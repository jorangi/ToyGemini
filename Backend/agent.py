import asyncio
import random
from pathlib import Path
import json
import json5
import uuid, datetime
from google.genai import types
from llm_utils import call_gemini_agent
from prompt_builder import build_prompt, extract_json_from_text
from sql_router import get_db_schema, Session, User, SessionLocal, ToolCallLog, save_message
from metatools.handlers import run_optimization_workflow, generate_tool_definition_and_code, register_newly_generated_tool, handle_reload_skills
from config import tool_manager, stream_text_by_char, safe_json_parse, MAX_AGENT_ITERATIONS, embedding_model, tool_collection, AGENT_STATE_PATH, load_model_priority
from optimization_manager import OptimizationManager
from tool_registry import ToolRegistry
from metatools.tool_selector import decide_tool, propose_parameters
from config import BACKEND_DIR
import re
from metatools.tool_selector import classify_goal_with_llm
from pathlib import Path
from dataclasses import dataclass
from typing import Optional, Deque, Dict, Any
from collections import deque
from pathlib import Path as _P
import os, json, re, hashlib, time
import inspect
import re, os, hashlib
from pathlib import Path
from dataclasses import dataclass
from collections import deque
from typing import Deque, Dict, Any
_PATH_RX = re.compile(
    r"""
    (                                   # Windows ì ˆëŒ€ê²½ë¡œ(C:\...) ë˜ëŠ” POSIX(/... , ./... , ../...)
        [A-Za-z]:[\\/](?:[^\\/:*?"<>|\r\n]+[\\/]?)+
        |
        (?:\.\.?[/\\])?(?:[/\\][^\\/:*?"<>|\r\n]+)+
    )
    """,
    re.VERBOSE,
)
from dataclasses import dataclass
from metatools.tool_selector import select_best_covering_tool

@dataclass
class _Effect:
    kind: str           # 'file_mutation' | 'file_backup' | 'unknown'
    ok: bool
    primary_path: str = ""
    aux_path: str = ""
    meta: dict | None = None

def _adapt_action_params_for(handler, action: str, params: dict) -> dict:
        """
        STRICT MODE:
        - í•¸ë“¤ëŸ¬ ì‹œê·¸ë‹ˆì²˜ì— ì •ì˜ëœ í‚¤ë§Œ í†µê³¼
        - ë™ì˜ì–´/ë³„ì¹­/ìë™ì¹˜í™˜ ê¸ˆì§€
        - **kwargs í—ˆìš© í•¸ë“¤ëŸ¬ëŠ” ê·¸ëŒ€ë¡œ í†µê³¼
        """
        if not isinstance(params, dict):
            return {}
        try:
            sig = inspect.signature(handler)
        except Exception:
            # ì‹œê·¸ë‹ˆì²˜ë¥¼ ëª» ì½ìœ¼ë©´ ìˆëŠ” ê·¸ëŒ€ë¡œ ì „ë‹¬ (í…ŒìŠ¤íŠ¸ í˜¸í™˜)
            return params

        # **kwargs í—ˆìš©ì´ë©´ í•„í„°ë§í•˜ì§€ ì•ŠìŒ
        if any(p.kind == inspect.Parameter.VAR_KEYWORD for p in sig.parameters.values()):
            return params

        expected = set(sig.parameters.keys())
        # ì •í™•íˆ ì¼ì¹˜í•˜ëŠ” í‚¤ë§Œ í†µê³¼
        return {k: v for k, v in params.items() if k in expected}

class Agent:
    def __init__(self, user_goal: str, session_id: str, background_tasks, model_priority_list: list):
        self.tool_manager = tool_manager
        self.user_goal = user_goal
        self.session_id = session_id
        self.background_tasks = background_tasks
        self.available_models = load_model_priority()
        self._backed_up_paths = set()
        self._written_paths = set()   # â† ìƒˆë¡œ ì¶”ê°€: ì´ë²ˆ ì„¸ì…˜ì—ì„œ 'ì‹¤ì œ ì“°ê¸°'ê°€ ë°œìƒí•œ ê²½ë¡œ ì¶”ì 
        self.request_id = str(uuid.uuid4())
        self.db = SessionLocal()
        self.prompt_content = []
        self.session_workflow = []
        self.last_written_file = None
        self.current_user_id = None
        self._backed_up_paths = set()
        self.tool_definitions = self._load_tool_definitions()
        self.optim_manager = OptimizationManager()
        self._materialize_queue = []
        self._effects: Deque[_Effect] = deque(maxlen=8)
        self._file_snapshots: Dict[str, tuple] = {}
        try:
            if AGENT_STATE_PATH.exists():
                with open(AGENT_STATE_PATH, 'r', encoding='utf-8') as f:
                    state = json.load(f)
                    self.available_models = state.get("model_priority_list", list(model_priority_list))
                    print(f"ğŸ’¡ ì €ì¥ëœ ëª¨ë¸ ìš°ì„ ìˆœìœ„ë¥¼ ë¡œë“œí–ˆìŠµë‹ˆë‹¤: {self.available_models}")
            else:
                self.available_models = list(model_priority_list)
        except Exception as e:
            print(f"âš ï¸ Agent ìƒíƒœ ë¡œë”© ì‹¤íŒ¨, ê¸°ë³¸ê°’ ì‚¬ìš©: {e}")
            self.available_models = list(model_priority_list)
    # ==== ë„£ì„ ìœ„ì¹˜: Agent í´ë˜ìŠ¤ ë‚´ë¶€(ì¤‘ë³µ ì •ì˜ ì •ë¦¬) ====
    from dataclasses import dataclass
    from pathlib import Path as _P
    import hashlib, os, time, re
    # --- Agent í´ë˜ìŠ¤ ë‚´ë¶€: ë‹¨ì¼ ì§„ì… í˜¸ì¶œ ë˜í¼ ---
    # --- Agent._call_tool: observation_resultëŠ” ì—¬ê¸°ì„œë§Œ ë§Œë“ ë‹¤ ---
    import inspect, time, json
    def _user_explicitly_requested_path(self, candidate: str) -> bool:
        """
        ì‚¬ìš©ìê°€ 'ë¬¸ì ê·¸ëŒ€ë¡œ' íŒŒì¼ ê²½ë¡œ/íŒŒì¼ëª…ì„ ìš”ì²­ì— í¬í•¨í–ˆì„ ë•Œë§Œ True.
        - ì ˆëŒ€ê²½ë¡œì´ê±°ë‚˜ ë””ë ‰í„°ë¦¬ êµ¬ë¶„ì í¬í•¨ â†’ ëª…ì‹œë¡œ ê°„ì£¼
        - ë˜ëŠ” user_goal ë‚´ì— ì •í™•íˆ ë™ì¼í•œ í† í°ì´ ì¡´ì¬
        """
        if not candidate or not isinstance(candidate, str):
            return False
        if os.path.isabs(candidate) or (os.sep in candidate or "/" in candidate):
            return True
        goal = (self.user_goal or "")
        # ê³µë°±/êµ¬ë‘ì  ê²½ê³„ì—ì„œ ì •í™•íˆ ì¼ì¹˜í•˜ëŠ”ì§€ í™•ì¸
        pat = r'(?<!\w)' + re.escape(candidate) + r'(?!\w)'
        return re.search(pat, goal) is not None
    def _get_planned_steps(self) -> list[str]:
        """
        ìµœì‹  í”Œëœì—ì„œ steps(action ì´ë¦„)ë§Œ ë½‘ì•„ì˜¨ë‹¤.
        LLMì´ ë‚¸ ê³„íšì´ ì—†ìœ¼ë©´, í˜„ì¬ ì•¡ì…˜ë§Œ ë‹¨ì¼ ìŠ¤í…ìœ¼ë¡œ ê°„ì£¼.
        """
        plan_obj = getattr(self, "_latest_plan_obj", None) or {}
        steps = []
        for s in (plan_obj.get("steps") or []):
            # step: {"action":"write_file", "parameters":{...}}
            a = (s or {}).get("action")
            if a:
                steps.append(a)
        return steps
    async def _call_tool(self, action: str, action_input: dict) -> dict:
        handler = getattr(self.tool_manager, "action_handlers", {}).get(action)
        if not handler:
            return {"status": "error", "reason": f"ì•Œ ìˆ˜ ì—†ëŠ” Agent ì•¡ì…˜: '{action}'."}

        # (ì„ íƒ) ì…ë ¥ ìŠ¤í‚¤ë§ˆ ê²€ì¦
        if hasattr(self, "_validate_action_input") and hasattr(self, "tool_definitions"):
            try:
                ok, schema_err = self._validate_action_input(action, action_input or {})
                if not ok:
                    # schema_err ìì²´ê°€ observation_result
                    print(f"[âœ… Observation] {json.dumps(schema_err, ensure_ascii=False, indent=2)}")
                    return schema_err
            except Exception as e:
                print(f"[Schema Validate Warning] {e}")

        # í•¸ë“¤ëŸ¬ ì‹œê·¸ë‹ˆì²˜ì— ë§ê²Œ íŒŒë¼ë¯¸í„° ì •ê·œí™”
        params = self._normalize_and_filter_params_for_handler(action, action_input or {})

        t0 = time.perf_counter()
        try:
            sig = inspect.signature(handler)
            # í•¸ë“¤ëŸ¬ê°€ action_input í•˜ë‚˜ë§Œ ë°›ëŠ” ê²½ìš° ì§€ì›
            if list(sig.parameters.keys()) == ["action_input"]:
                observation_result = await handler(params if "action_input" not in params else params["action_input"])
            else:
                observation_result = await handler(**params)
        except TypeError as e:
            # ì¸ì ë¶ˆì¼ì¹˜
            try:
                sig_str = str(inspect.signature(handler))
            except Exception:
                sig_str = "(signature unavailable)"
            observation_result = {
                "status": "error",
                "reason": f"í•¸ë“¤ëŸ¬ ì¸ì ë¶ˆì¼ì¹˜ë¡œ ì‹¤í–‰ ì‹¤íŒ¨: {e}",
                "handler_signature": sig_str,
                "given_params": action_input or {},
            }
        except Exception as e:
            # ì‹¤í–‰ ì¤‘ ì˜ˆì™¸
            observation_result = {"status": "error", "reason": f"í•¸ë“¤ëŸ¬ ì‹¤í–‰ ì¤‘ ì˜ˆì™¸: {e}"}

        # ê´€ì¸¡ ë¡œê·¸ ì¶œë ¥
        try:
            print(f"[âœ… Observation] {json.dumps(observation_result, ensure_ascii=False, indent=2)}")
        except Exception:
            print(f"[âœ… Observation] {observation_result}")

        # ì‚¬í›„ ì²˜ë¦¬(íŒŒì¼ ê²½ë¡œ ì¶”ì /ë³´ê°• í›… ë“±) â€” ì—¬ê¸°ì„œ observation_resultë§Œ ë„˜ê¹€
        try:
            await self._on_action_observed(action, action_input or {}, observation_result or {})
        except Exception as e:
            print(f"[Post-Observe] error: {e}")

        return observation_result or {}
    @staticmethod
    def _file_sig(path: str) -> tuple[int, int, str]:
        """íŒŒì¼ ë³€ê²½ ê°ì§€ë¥¼ ìœ„í•œ ê°€ë²¼ìš´ ì‹œê·¸ë‹ˆì²˜."""
        try:
            st = os.stat(path)
            size = st.st_size
            mtime = int(st.st_mtime)
            h = hashlib.sha1()
            with open(path, "rb") as f:
                h.update(f.read(65536))
                if size > 65536:
                    f.seek(max(0, size - 65536))
                    h.update(f.read(65536))
            return (size, mtime, h.hexdigest())
        except Exception:
            return (0, 0, "")
    @staticmethod
    def _is_auto_placeholder(text: str) -> bool:
        return isinstance(text, str) and re.fullmatch(r"\s*<\s*AUTO[^>]*>\s*", text or "") is not None
    @staticmethod
    def _normalize_for_dup(self, s: str) -> str:
        # ë¬¸ë‹¨ ì¤‘ë³µ ì œê±°ìš© ê°„ë‹¨ ì •ê·œí™”
        return re.sub(r"\s+", " ", re.sub(r"[^\wê°€-í£]+", "", (s or "").lower())).strip()
    @staticmethod
    def _dedupe_paragraphs(text: str) -> str:
        """ì™„ì „ ë™ì¼ ë¬¸ë‹¨ ë°˜ë³µ ë°©ì§€. ìì—°ì–´ í‚¤ì›Œë“œ ë¶ˆì‚¬ìš©."""
        if not isinstance(text, str):
            return text
        paras = re.split(r"(?:\r?\n){2,}", text)
        seen, out = set(), []
        for p in paras:
            key = p.strip()
            if key and key not in seen:
                seen.add(key); out.append(p)
        return "\n\n".join(out)
    @staticmethod
    def _looks_write_success(observation: dict) -> bool:
        if not isinstance(observation, dict) or observation.get("status") != "ok":
            return False
        det = observation.get("detail", {}) if isinstance(observation.get("detail"), dict) else {}
        return bool(observation.get("path") or (isinstance(det, dict) and det.get("path")))
    @staticmethod
    def _looks_backup_success(observation: dict) -> bool:
        if not isinstance(observation, dict) or observation.get("status") != "ok":
            return False
        det = observation.get("detail", {}) if isinstance(observation.get("detail"), dict) else {}
        src = observation.get("src") or (det.get("src") if isinstance(det, dict) else None)
        bak = observation.get("backup_path") or (det.get("backup_path") if isinstance(det, dict) else None)
        return bool(src and bak)
    async def _plan_with_retry(self, user_goal_text: str, max_attempts: int = 2) -> dict | None:
        """
        í†µí•© ê³„íšì„ LLMìœ¼ë¡œë¶€í„° JSONìœ¼ë¡œ ë°›ì•„ì˜¤ë˜, ì‹¤íŒ¨ ì‹œ 1íšŒ ì¬ì‹œë„.
        - 1ì°¨: í˜„ì¬ ê¸°ë³¸ ëª¨ë¸/ì„¤ì •
        - 2ì°¨: ë” ë³´ìˆ˜ì ì¸ ì„¤ì •(ì˜¨ë„ 0, ë‹¤ë¥¸ ìš°ì„ ìˆœìœ„ ëª¨ë¸)ë¡œ ì¬ì‹œë„
        ë°˜í™˜: ê³„íš ê°ì²´(dict) ë˜ëŠ” None
        """
        # í”„ë¡¬í”„íŠ¸ëŠ” ê¸°ì¡´ í”Œë˜ë„ˆì—ì„œ ì“°ë˜ â€˜ê³„íš JSONâ€™ ì§€ì‹œë¬¸ì„ ê·¸ëŒ€ë¡œ ì¬ì‚¬ìš©í•˜ë˜,
        # ì—¬ê¸°ì„œëŠ” ë³„ë„ í•˜ë“œì½”ë”© ì—†ì´, ë‹¨ì§€ JSON íŒŒì‹± ì„±ê³µ ì—¬ë¶€ë§Œ ë³¸ë‹¤.
        base_instruction = (
            "Produce a valid JSON plan object. Do not include any non-JSON text.\n"
            "The JSON must parse with a standard parser."
        )
        for attempt in range(1, max_attempts + 1):
            try:
                use_tools = False
                available = getattr(self, "available_models", None)
                extra = {}
                if attempt == 2:
                    # ë‘ ë²ˆì§¸ ì‹œë„: ë” ë³´ìˆ˜ì ì¸ ë””ì½”ë”©ì„ ìœ„í•´ ì˜µì…˜ë§Œ ì‚´ì§ ë³´ìˆ˜ì ìœ¼ë¡œ
                    extra["temperature"] = 0
                    # ëª¨ë¸ ìš°ì„ ìˆœìœ„ë¥¼ ë‚´ë¶€ì ìœ¼ë¡œ ë°”ê¿€ ìˆ˜ ìˆë‹¤ë©´ _call_llm_safeê°€ ì´ë¥¼ ë°˜ì˜

                prompt = f"{base_instruction}\n\nUSER_GOAL:\n{user_goal_text or ''}"
                text, _ = await self._call_llm_safe(
                    [self._build_user_message(prompt)],
                    available_models=available,
                    use_tools=use_tools,
                    **extra
                )
                plan = json.loads(text.strip())
                if isinstance(plan, dict):
                    return plan
            except Exception:
                continue
        return None
    def _messages_fingerprint(self, messages) -> str:
        """
        LLM í˜¸ì¶œ ë©”ì‹œì§€(ë¦¬ìŠ¤íŠ¸)ë¥¼ ì•ˆì •ì ìœ¼ë¡œ ì§ë ¬í™”í•´ í•´ì‹œ í‚¤ ìƒì„±.
        role/text/partsë§Œ ë³´ì¡´(ë‚´ìš© ë™ì¼í•˜ë©´ ë™ì¼ í‚¤).
        """
        def norm_msg(m):
            role = ""
            text = ""
            try:
                role = getattr(m, "role", "") or m.get("role", "")
                parts = getattr(m, "parts", None) or m.get("parts", [])
                buf = []
                for p in parts:
                    t = getattr(p, "text", None)
                    if t is None and isinstance(p, dict):
                        t = p.get("text")
                    if isinstance(t, str):
                        buf.append(t)
                text = "\n".join(buf)
            except Exception:
                text = str(m)
            return {"role": role, "text": text}

        try:
            norm = [norm_msg(m) for m in (messages or [])]
            payload = json.dumps(norm, ensure_ascii=False, sort_keys=True)
        except Exception:
            payload = str(messages)
        return hashlib.sha256(payload.encode("utf-8")).hexdigest()

    def _parse_retry_delay_sec(self, exc) -> int:
        """
        ì˜ˆì™¸ payloadì—ì„œ 'retryDelay': '18s' ê°™ì€ ê°’ì„ ì¶”ì¶œ. ì‹¤íŒ¨í•˜ë©´ ê¸°ë³¸ 5ì´ˆ.
        """
        txt = ""
        try:
            txt = str(getattr(exc, "args", [""])[-1]) or ""
        except Exception:
            pass
        m = re.search(r"retryDelay[^0-9]*([0-9]+)\s*s", txt)
        if m:
            try:
                return max(1, int(m.group(1)))
            except Exception:
                pass
        return 5
    def _extract_written_file_path(self, action: str, action_input: dict, observation: dict) -> str | None:
        det = observation.get("detail", {}) if isinstance(observation.get("detail"), dict) else {}
        p = det.get("path") or observation.get("path")
        if p: return str(p)
        for k in ("file_path", "path", "target_path"):
            if k in (action_input or {}): return str(action_input[k])
        return getattr(self, "last_written_file", None)
    def _normalize_and_filter_params_for_handler(self, action: str, raw: dict) -> dict:
        params = dict(raw or {})
        handler = getattr(self.tool_manager, "action_handlers", {}).get(action)
        if not handler:
            return params
        try:
            sig = inspect.signature(handler)
            expected = set(sig.parameters.keys())
        except Exception:
            return params
        if "action_input" in expected:
            return {"action_input": params}
        return {k: v for k, v in params.items() if k in expected}
    def _normalize_effect(self, *args, **kwargs) -> _Effect:
        """
        í˜¸ì¶œë¶€ í˜¸í™˜: _normalize_effect(observation) ë˜ëŠ” _normalize_effect(action, observation) ë‘˜ ë‹¤ í—ˆìš©.
        """
        if len(args) == 1 and isinstance(args[0], dict):
            observation = args[0]
        elif len(args) >= 2 and isinstance(args[1], dict):
            observation = args[1]
        else:
            return _Effect()

        ok = (observation.get("status") == "ok") if isinstance(observation, dict) else False
        det = observation.get("detail", {}) if isinstance(observation.get("detail"), dict) else {}
        if isinstance(det, dict):
            ok = ok or (det.get("status") in ("ok", "success") or det.get("success") is True)

        path = (isinstance(det, dict) and det.get("path")) or observation.get("path") or ""
        src  = observation.get("src") or (det.get("src") if isinstance(det, dict) else "")
        bkp  = observation.get("backup_path") or (det.get("backup_path") if isinstance(det, dict) else "")

        # ë°±ì—… ì„±ê³µ
        if ok and src and bkp:
            return _Effect(kind="file_backup", ok=True, primary_path=str(src), aux_path=str(bkp), meta=observation)

        # ì“°ê¸° ì„±ê³µ + ì‹¤ì œ ë³€ê²½
        if ok and path:
            sig_prev = getattr(self, "_file_snapshots", {}).get(path) if hasattr(self, "_file_snapshots") else None
            sig_now  = self._file_sig(path)
            if not hasattr(self, "_file_snapshots"): self._file_snapshots = {}
            self._file_snapshots[path] = sig_now
            if (sig_prev is None) or (sig_prev != sig_now):
                return _Effect(kind="file_mutation", ok=True, primary_path=str(path), meta=observation)

        return _Effect(kind="unknown", ok=ok, primary_path=str(path), meta=observation)
    async def _on_action_observed(self, action: str, action_input: dict, observation: dict):
        """
        1) write_file ì„±ê³µ ì‹œ self.last_written_file ê°±ì‹ 
        2) backup_file ì„±ê³µ ì‹œ ìµœê·¼ ì¤‘ë³µ ë°±ì—… ì–µì œ(ì„ íƒì  í”¼ë²—)
        3) í›„ì† ë¶€ìŠ¤íŠ¸: _post_write_boost(action, action_input, observation)
        """
        try:
            det = observation.get("detail", {}) if isinstance(observation.get("detail"), dict) else {}
            # (1) ìµœê·¼ ì‘ì„± íŒŒì¼ ê²½ë¡œ ê¸°ì–µ
            if self._looks_write_success(observation):
                self.last_written_file = (det.get("path") or observation.get("path") or "") or self.last_written_file

            # (2) ë°±ì—… ì„±ê³µ ì‹œ, ë„ˆë¬´ ì´˜ì´˜í•œ ì¤‘ë³µ ë°±ì—…ì´ë©´ ë§¤í¬ë¡œë¡œ í”¼ë²— í›„ë³´
            if self._looks_backup_success(observation):
                primary = observation.get("src") or det.get("src")
                backup  = observation.get("backup_path") or det.get("backup_path")
                if primary and backup:
                    now = time.time()
                    last = getattr(self, "_last_backup_ts", 0.0)
                    if now - last < 0.8:
                        # ì§í›„ ë‹¨ê³„ì—ì„œ ë§¤í¬ë¡œë¡œ í•©ì¹˜ê¸° ì‹œë„ (ì„ íƒ)
                        try:
                            action, action_input, meta = self._maybe_override_with_registry_tool(action, action_input)
                            if meta.get("overridden"):
                                print(f"[Registry Override] {meta}")
                        except Exception as _e:
                            print(f"[BackupThrottle Warning] {str(_e)}")
                    self._last_backup_ts = now

            # (3) í›„ì† ë¶€ìŠ¤íŠ¸ â€” ì˜ëª»ëœ ì´ì¤‘ self ì „ë‹¬ ì œê±°
            await self._post_write_boost(action, action_input, observation)
        except Exception as e:
            print(f"[Post-Observe] error: {e}")
    # --- BEGIN: Back-compat shim for tests expecting `_adapt_action_params_for` ---
    import inspect  # ìƒë‹¨ì— ì´ë¯¸ ìˆë‹¤ë©´ ì¤‘ë³µ import ë¬´ê´€
    if not callable(globals().get("_adapt_action_params_for", None)):
        def _adapt_action_params_for(handler, action: str, params: dict) -> dict:
            """
            STRICT MODE:
            - í•¸ë“¤ëŸ¬ ì‹œê·¸ë‹ˆì²˜ì— ì •ì˜ëœ í‚¤ë§Œ í†µê³¼
            - ë™ì˜ì–´/ë³„ì¹­/ìë™ì¹˜í™˜ ê¸ˆì§€
            - **kwargs í—ˆìš© í•¸ë“¤ëŸ¬ëŠ” ê·¸ëŒ€ë¡œ í†µê³¼
            """
            if not isinstance(params, dict):
                return {}
            try:
                sig = inspect.signature(handler)
            except Exception:
                # ì‹œê·¸ë‹ˆì²˜ë¥¼ ëª» ì½ìœ¼ë©´ ìˆëŠ” ê·¸ëŒ€ë¡œ ì „ë‹¬ (ë³´ìˆ˜ì  í˜¸í™˜)
                return params
            # **kwargs í—ˆìš©ì´ë©´ í•„í„°ë§í•˜ì§€ ì•ŠìŒ
            if any(p.kind == inspect.Parameter.VAR_KEYWORD for p in sig.parameters.values()):
                return params
            expected = set(sig.parameters.keys())
            # ì •í™•íˆ ì¼ì¹˜í•˜ëŠ” í‚¤ë§Œ í†µê³¼
            return {k: v for k, v in params.items() if k in expected}
    # --- END: Back-compat shim ---
    async def _llm_decide_destination(self) -> str:
        """
        LLMì—ê²Œ 'ì¶œë ¥ ëª©ì ì§€ê°€ ë¬´ì—‡ì¸ì§€'ë¥¼ JSONìœ¼ë¡œ íŒë‹¨ë°›ëŠ”ë‹¤.
        - ë°˜í™˜: "inline" | "file"
        - ê·œì¹™: ì‚¬ìš©ìê°€ 'ë³„ë„ì˜ íŒŒì¼ë¡œ ì €ì¥'ì„ ëª…ì‹œí•˜ì§€ ì•Šìœ¼ë©´ "inline"
        - ì–¸ì–´/í‘œí˜„ ë°©ì‹ì— ìƒê´€ì—†ì´ LLMì´ ì˜ë¯¸ë¡œ íŒë‹¨ (í‚¤ì›Œë“œ ë¦¬ìŠ¤íŠ¸ ì—†ìŒ)
        """
        user_goal = self.user_goal or ""
        instruction = (
            "You are a strict JSON decision maker.\n"
            "Task: Determine the destination for result delivery.\n"
            "Rules:\n"
            "1) If the user explicitly requests to save as a separate file (persistent artifact), destination = \"file\".\n"
            "2) Otherwise, destination = \"inline\".\n"
            "3) Do not infer intent from vague hints; only explicit file-saving requests count as \"file\".\n"
            "Return JSON only with key: {\"destination\": \"inline\" | \"file\"}."
        )
        prompt = f"{instruction}\n\nUSER_REQUEST:\n{user_goal}"
        try:
            response_text, _ = await self._call_llm_safe(
                [self._build_user_message(prompt)],
                available_models=getattr(self, "available_models", None),
                use_tools=False
            )
            data = json.loads(response_text.strip())
            dest = data.get("destination", "").strip().lower()
            if dest in ("inline", "file"):
                return dest
            return "inline"
        except Exception:
            return "inline"

    async def _llm_author_parameters(self, *, tool_name: str, tool_spec: dict, destination: str, proposed_params: dict) -> dict:
        """
        LLMì—ê²Œ 'ì„ íƒëœ ë„êµ¬ì˜ íŒŒë¼ë¯¸í„°'ë¥¼ ìµœì¢… ì‘ì„±ì‹œí‚¤ëŠ” ë‹¨ê³„.
        - tool_spec: ì„ íƒëœ ë„êµ¬ì˜ ì „ì²´ ìŠ¤í‚¤ë§ˆ(JSON). (ë„êµ¬ëª…/íŒŒë¼ë¯¸í„°ëª… í•˜ë“œì½”ë”© ì—†ìŒ)
        - destination: "inline" ë˜ëŠ” "file"
        - proposed_params: í”Œë˜ë„ˆê°€ ì œì•ˆí•œ íŒŒë¼ë¯¸í„°(ì´ˆì•ˆ). LLMì´ ì°¸ê³ í•˜ì—¬ ì •ì œ.

        ê·œì¹™(ë¬¸ì„œí™”ë§Œ; ì½”ë“œë¡œ íŠ¹ì • í‚¤ ì°¾ì§€ ì•ŠìŒ):
        - destination == inline: ë³„ë„ íŒŒì¼ ê²½ë¡œ ì§€ì •í•˜ì§€ ë§ê³ , ê¸°ë³¸ ì‹±í¬ë¥¼ ì‚¬ìš©í•˜ë„ë¡ ì„¤ê³„í•˜ë¼.
        - destination == file: ì‚¬ìš©ìê°€ ëª…ì‹œí•œ ì €ì¥ ê²½ë¡œê°€ ì—†ë‹¤ë©´, í•©ë¦¬ì ì¸ ê¸°ë³¸ ì‚°ì¶œë¬¼ ê²½ë¡œë¥¼ ì œì•ˆí•´ë„ ëœë‹¤.

        ë°˜í™˜: í•´ë‹¹ ë„êµ¬ ìŠ¤í‚¤ë§ˆì— ë§ëŠ” 'ìµœì¢… íŒŒë¼ë¯¸í„°' JSON ë”•ì…”ë„ˆë¦¬
        """
        # ê¸°ë³¸ ì‹±í¬ ê²½ë¡œ ìƒìˆ˜ëŠ” ì„¤ì •ì—ì„œ ê°€ì ¸ì™€ LLMì— 'ì„¤ëª…'ìœ¼ë¡œë§Œ ì „ë‹¬
        try:
            from tool_registry import DEFAULT_OUTPUT_PATH
        except Exception:
            DEFAULT_OUTPUT_PATH = "Frontend/public/longText.txt"

        # LLMì— ì œê³µí•  ì»¨í…ìŠ¤íŠ¸
        tool_schema_json = json.dumps(tool_spec or {}, ensure_ascii=False, indent=2)
        proposed_json = json.dumps(proposed_params or {}, ensure_ascii=False, indent=2)

        instruction = (
            "You are a strict JSON parameter author for a selected tool.\n"
            "You will be given:\n"
            " - TOOL_SCHEMA: the selected tool's full JSON schema (including parameters).\n"
            " - DESTINATION: either \"inline\" or \"file\".\n"
            " - DEFAULT_SINK: the default sink path for inline delivery.\n"
            " - PROPOSED_PARAMS: a draft parameter object from a planner.\n"
            "\n"
            "Your job:\n"
            " - Produce a minimal JSON object that matches the TOOL_SCHEMA.parameters shape.\n"
            " - If DESTINATION == \"inline\": avoid proposing any separate artifact path; instead, rely on the tool's defaults to use DEFAULT_SINK.\n"
            " - If DESTINATION == \"file\": you may include an explicit path only if the user's request clearly specified it; otherwise propose a reasonable output path.\n"
            " - Never add fields not present in TOOL_SCHEMA.parameters.properties.\n"
            " - Preserve essential content from PROPOSED_PARAMS if present (e.g., the main body of text/code), but do not invent file names unless DESTINATION == \"file\".\n"
            " - Return JSON only, no prose.\n"
        )

        user_goal = self.user_goal or ""
        prompt = (
            f"{instruction}\n\n"
            f"USER_REQUEST:\n{user_goal}\n\n"
            f"DESTINATION:\n{destination}\n\n"
            f"DEFAULT_SINK:\n{DEFAULT_OUTPUT_PATH}\n\n"
            f"TOOL_SCHEMA:\n{tool_schema_json}\n\n"
            f"PROPOSED_PARAMS:\n{proposed_json}\n"
        )

        try:
            response_text, _ = await self._call_llm_safe(
                [self._build_user_message(prompt)],
                available_models=getattr(self, "available_models", None),
                use_tools=False
            )
            params = json.loads(response_text.strip())
            if isinstance(params, dict):
                return params
        except Exception:
            pass

        # LLM ì‹¤íŒ¨ ì‹œ, ìµœì†Œ íŒŒë¼ë¯¸í„° í´ë°±: ë‚´ìš©ë§Œ ë³´ì¡´ (ê²½ë¡œ ê´€ë ¨ ì…ë ¥ì€ í¬í•¨í•˜ì§€ ì•ŠìŒ)
        minimal = {}
        # proposed_params ì•ˆì— 'ë‚´ìš©ì— í•´ë‹¹í•˜ëŠ” ê°’'ì´ ìˆì„ ìˆ˜ ìˆìœ¼ë‹ˆ, ê°€ì¥ í° í…ìŠ¤íŠ¸ ê°’ í•˜ë‚˜ë¥¼ ì¡ì•„ë³¸ë‹¤(í‚¤ ì´ë¦„ ê°€ì • ì—†ì´).
        try:
            longest_text_key = None
            longest_len = -1
            for k, v in (proposed_params or {}).items():
                if isinstance(v, str) and len(v) > longest_len:
                    longest_len = len(v)
                    longest_text_key = k
            if longest_text_key is not None:
                minimal[longest_text_key] = proposed_params[longest_text_key]
        except Exception:
            pass
        return minimal
    async def _dispatch_action(self, action: str, action_input: dict):
        """
        ì¤‘ì•™ ë””ìŠ¤íŒ¨ì²˜(ê¶Œì¥): ì—¬ê¸°ì„œ íŒŒë¼ë¯¸í„° í˜¸í™˜ â†’ ë°±ì—… í”¼ë²— â†’ ì‹¤ì œ í˜¸ì¶œ â†’ ê´€ì¸¡ í›… í˜¸ì¶œì„ ê³ ì •í•©ë‹ˆë‹¤.
        ë©”ì¸ ë£¨í”„ì—ì„œ handler ì§ì ‘ ë¶€ë¥´ì§€ ë§ê³  ì´ í•¨ìˆ˜ë¥¼ í†µí•´ í˜¸ì¶œí•˜ì„¸ìš”.
        """
        # 1) íŒŒë¼ë¯¸í„° í˜¸í™˜(í•¸ë“¤ëŸ¬ ì‹œê·¸ë‹ˆì²˜ ë¶ˆì¼ì¹˜ ë°©ì§€)
        action_input = self._adapt_action_params(self, action, action_input)

        # 2) ë°±ì—… ë£¨í”„ í”¼ë²—(ë™ì¼ íŒŒì¼ ë°±ì—…ë§Œ ë°˜ë³µ ì‹œ ì“°ê¸° ê³„ì—´ë¡œ ì „í™˜)
        if hasattr(self, "_pivot_duplicate_backup"):
            try:
                action, action_input = await self._pivot_duplicate_backup(action, action_input)
                action_input = self._adapt_action_params(self, action, action_input)
            except Exception as e:
                print(f"[BackupPivot Warning] {e}")

        # 3) ì‹¤ì œ ë„êµ¬ í˜¸ì¶œ
        handler = self.tool_manager.action_handlers.get(action)
        if not handler:
            raise RuntimeError(f"unknown tool: {action}")
        observation = await handler(**(action_input or {}))

        # 4) ê´€ì¸¡ í›„ì²˜ë¦¬(íŒŒì¼ ê²½ë¡œ ì¶”ì /ì´ì–´ì“°ê¸° í›…)
        await self._on_action_observed(self, action, action_input, observation)
        return observation
    async def _post_write_boost(self, tool_name: str, action_input: dict, observation: dict) -> None:
        """
        - ëª©í‘œê°€ longform_textë©´ ë¶„ëŸ‰ ë¶€ì¡±ë¶„ì„ append.
        - ë¹„ì–´ìˆê±°ë‚˜ <AUTO...> í”Œë ˆì´ìŠ¤í™€ë”ë§Œ ìˆìœ¼ë©´ 'ì²˜ìŒ ë³¸ë¬¸'ë¶€í„° ìƒì„±.
        - ë„êµ¬ëª… í•˜ë“œì½”ë”©/ìì—°ì–´ í‚¤ì›Œë“œ ë§¤ì¹­ ì—†ìŒ.
        """
        try:
            cls = getattr(self, "_goal_classification", {}) or {}
            if cls.get("content_kind") != "longform_text":
                return

            # ì–´ë””ì— ì¼ëŠ”ì§€ ì¶”ì 
            path = self._extract_written_file_path(tool_name, action_input or {}, observation or {}) \
                or getattr(self, "last_written_file", "")
            if not path:
                return

            try:
                cur = Path(path).read_text(encoding="utf-8", errors="ignore")
            except Exception:
                cur = ""

            # ë¶„ëŸ‰ íŒíŠ¸
            try:
                from metatools.tool_selector import estimate_length_hint
                min_chars, target_chars = estimate_length_hint(self.user_goal or "", "longform_text")
            except Exception:
                min_chars, target_chars = 1500, 2500

            # ì´ˆê¸° ë³¸ë¬¸ ì—¬ë¶€
            cur_is_emptyish = (not cur.strip()) or self._is_auto_placeholder(cur)

            # ë©”ì‹œì§€ êµ¬ì„±(LLM ì‹œê·¸ë‹ˆì²˜ ë³€í™” ì•ˆì „ í˜¸ì¶œ)

            _msgs = self._build_user_message
            if cur_is_emptyish:
                prompt = (
                    "ìš”ì²­ì— ë¶€í•©í•˜ëŠ” í•œêµ­ì–´ ì¥ë¬¸ ë³¸ë¬¸ì„ ì§ì ‘ ì‘ì„±í•˜ì„¸ìš”.\n"
                    "- ë©”íƒ€ ì„¤ëª…/ì‚¬ê³¼/ì•ˆë‚´ë¬¸ ê¸ˆì§€, ë³¸ë¬¸ë§Œ.\n"
                    "- ì¤‘ë³µ ì—†ì´ ìì—°ìŠ¤ëŸ½ê²Œ ì„œì‚¬/ì„¤ëª…/ë¬˜ì‚¬ë¥¼ í¬í•¨.\n"
                    "- ê¸¸ì´ ëª©í‘œ: ìµœì†Œ {min_c}ì, ê°€ê¸‰ì  {tgt_c}ì ë¶€ê·¼.\n"
                ).format(min_c=min_chars, tgt_c=target_chars)
                resp, _ = await self._call_llm_safe(_msgs(prompt), available_models=getattr(self, "available_models", None), use_tools=False)
                add = (getattr(resp, "text", None) or "").strip()
                if not add: return
                new_text = add
            else:
                prompt = (
                    "ë‹¤ìŒ í•œêµ­ì–´ ë³¸ë¬¸ì„ ìì—°ìŠ¤ëŸ½ê²Œ 'ì´ì–´ì§€ëŠ” ë‹¤ìŒ ë¶€ë¶„'ìœ¼ë¡œ í™•ì¥í•˜ì„¸ìš”.\n"
                    "- ì´ë¯¸ ìˆëŠ” ë¬¸ì¥/ë¬¸ë‹¨ì„ ë°˜ë³µí•˜ê±°ë‚˜ ìš”ì•½í•˜ì§€ ë§ ê²ƒ.\n"
                    "- ìƒˆ ì‚¬ê±´/ì„¸ë¶€/ê°ì • ë¬˜ì‚¬ë¡œ ë¶„ëŸ‰ì„ í™•ì¥.\n"
                    "- ë©”íƒ€ ì½”ë©˜íŠ¸ ì—†ì´ ì‹¤ì œ ë³¸ë¬¸ë§Œ ì¶œë ¥.\n\n"
                    "[ì´ì „ ë³¸ë¬¸ (ë§ë¯¸ 4000ì)]\n" + cur[-4000:]
                )
                resp, _ = await self._call_llm_safe(_msgs(prompt), available_models=getattr(self, "available_models", None), use_tools=False)
                add = (getattr(resp, "text", None) or "").strip()
                if not add: return
                add = self._dedupe_paragraphs(cur, add)
                if not add: return
                new_text = ("\n\n".join([cur.strip(), add]) if cur.strip() else add)

            combined = (cur.rstrip() + "\n\n" + new_text) if cur.strip() else new_text

            if "append_file" in self.tool_manager.action_handlers and cur.strip():
                await self.tool_manager.action_handlers["append_file"](file_path=path, content="\n\n" + new_text)
            elif "write_file" in self.tool_manager.action_handlers:
                # âš ï¸ ì ˆëŒ€ ìƒˆ í…ìŠ¤íŠ¸ë§Œ ì“°ì§€ ë§ê³  í•­ìƒ ë³‘í•©í•´ì„œ write
                await self.tool_manager.action_handlers["write_file"](file_path=path, content=combined)
            # self._written_paths ì—…ë°ì´íŠ¸ë„ ëˆ„ë½ ì—†ì´
            try:
                norm = str(_P(path).resolve())
                self._written_paths.add(norm)
            except Exception:
                pass

            # append/write ì•ˆì „ ì²˜ë¦¬(í•¸ë“¤ëŸ¬ ì¡´ì¬ì‹œ ìš°ì„  í™œìš©)
            if "append_file" in self.tool_manager.action_handlers and not cur_is_emptyish:
                await self.tool_manager.action_handlers["append_file"](file_path=path, content="\n\n" + (new_text[len(cur):] if new_text.startswith(cur) else new_text))
            elif "write_file" in self.tool_manager.action_handlers:
                await self.tool_manager.action_handlers["write_file"](file_path=path, content=new_text)
            else:
                Path(path).parent.mkdir(parents=True, exist_ok=True)
                with open(path, "w", encoding="utf-8") as f:
                    f.write(new_text)
            try:
                size = len(_P(path).read_text(encoding="utf-8", errors="ignore"))
                print(f"[PostWrite] file='{path}' size={size}")
            except Exception as _e:
                print(f"[PostWrite] size check failed: {str(_e)}")

        except Exception as e:
            print(f"[PostWrite] continuation error: {e}")
    from pathlib import Path
    import time
    def _norm_path(self, p):
        try:
            return str(Path(p).resolve())
        except Exception:
            return p or ""
    def _extract_primary_aux(self, action: str, observation: dict):
        """ê´€ì°° ê²°ê³¼ë§Œìœ¼ë¡œ ì£¼/ë³´ì¡° ê²½ë¡œë¥¼ ì¶”ì¶œ(ë„êµ¬ëª… ìƒê´€ì—†ìŒ)."""
        det = observation.get("detail", {})
        if not isinstance(det, dict): det = {}

        path  = observation.get("path") or det.get("path")
        src   = observation.get("src") or det.get("src")
        bpath = observation.get("backup_path") or det.get("backup_path")

        # ë°±ì—…ì´ë©´ ì›ë³¸ì´ primary, ë°±ì—…ë³¸ì´ aux
        if src and bpath:
            primary = src
            aux = bpath
        else:
            primary = path or src
            aux = bpath

        return self._norm_path(primary), self._norm_path(aux)
    async def _on_action_observed(self, action: str, action_input: dict, observation: dict):
        """ì•¡ì…˜ ìˆ˜í–‰ ì§í›„ 1íšŒ: ìƒíƒœ ê°±ì‹  + ì´ì–´ì“°ê¸° íŠ¸ë¦¬ê±° + ë°±ì—… ìŠ¤ë¡œí‹€."""
        try:
            # ë§ˆì§€ë§‰ìœ¼ë¡œ ìˆ˜ì •ëœ íŒŒì¼ ì¶”ì 
            if self._looks_write_success(observation):
                det = observation.get("detail", {}) if isinstance(observation.get("detail"), dict) else {}
                self.last_written_file = (det.get("path") or observation.get("path") or "")

            # ë°±ì—… ìŠ¤ë¡œí‹€(60ì´ˆ ë‚´ ë°˜ë³µ)
            if not hasattr(self, "_recent_backup_ts"):
                self._recent_backup_ts = {}
            if self._looks_backup_success(observation):
                primary = observation.get("src") or (det.get("src") if isinstance(det, dict) else None)
                if primary:
                    import time as _t
                    now = _t.time()
                    prev = self._recent_backup_ts.get(primary)
                    self._recent_backup_ts[primary] = now
                    if prev and (now - prev) < 60:
                        print(f"[BackupThrottle] skip repetitive backup hint for {primary}")

            # íŒŒì¼ ì‘ì„± ì„±ê³µì‹œì—ë§Œ ì´ì–´ì“°ê¸° í›„í¬
            if self._looks_write_success(observation):
                await self._post_write_boost(action, action_input or {}, observation or {})
        except Exception as e:
            print(f"[Post-Observe] error: {e}")
    async def _post_write_boost(self, tool_name: str, action_input: dict, observation: dict) -> None:
        """
        íŒŒì¼ ì‘ì„± ì§í›„ ë³´ê°•:
        - ëª©í‘œê°€ ì¥ë¬¸ì´ë©´ ë¶„ëŸ‰ ë¶€ì¡±ë¶„ì„ append.
        - ì´ˆê¸° ë³¸ë¬¸ì´ ì§§ìœ¼ë©´ 'ì²˜ìŒ ë³¸ë¬¸'ë¶€í„° ìƒì„±(ì´ì–´ì“°ê¸° ì•„ë‹˜).
        - ìì—°ì–´ í‚¤ì›Œë“œ ë§¤ì¹­ ì—†ì´ ê¸¸ì´/êµ¬ì¡°ë§Œ ì‚¬ìš©.
        """
        try:
            from pathlib import Path as _P
            cls = getattr(self, "_goal_classification", {}) or {}
            if cls.get("content_kind") != "longform_text":
                return

            det = observation.get("detail", {}) if isinstance(observation.get("detail"), dict) else {}
            path = det.get("path") or observation.get("path") or getattr(self, "last_written_file", "")
            if not path:
                return

            try:
                cur = _P(path).read_text(encoding="utf-8", errors="ignore")
            except Exception:
                cur = ""

            # ë¶„ëŸ‰ íŒíŠ¸
            try:
                from metatools.tool_selector import estimate_length_hint
                min_chars, max_chars = estimate_length_hint(self.user_goal or "", "longform_text")
            except Exception:
                # ì•ˆì „ ê¸°ë³¸ê°’(ì˜ˆ: 1500ì ~ 2500ì)
                min_chars, max_chars = 1500, 2500

            # --- ì´ˆê¸° ë³¸ë¬¸ì´ ë¹„ê±°ë‚˜ ë„ˆë¬´ ì§§ìœ¼ë©´ 'ì²˜ìŒ ë³¸ë¬¸' ìƒì„± ---
            cur_len = len(cur.strip())
            if cur_len < max(50, min_chars // 10):
                from google.genai import types
                seed = (
                    "ìš”ì²­ì— ë§ëŠ” í•œêµ­ì–´ ì¥ë¬¸ ë³¸ë¬¸ì„ ì‘ì„±í•˜ì„¸ìš”.\n"
                    "- ë©”íƒ€ ì½”ë©˜íŠ¸ ì—†ì´ ë³¸ë¬¸ë§Œ ì¶œë ¥\n"
                    "- ë„ì…-ì „ê°œ-í´ë¼ì´ë§¥ìŠ¤ íë¦„ìœ¼ë¡œ ìì—°ìŠ¤ëŸ½ê²Œ ì‹œì‘\n"
                )
                goal = (self.user_goal or "").strip()
                if goal:
                    seed += "\n[ìš”ì²­]\n" + goal
                resp, _ = await self._call_llm_safe(
                    [types.Content(role="user", parts=[types.Part(text=seed)])],
                    available_models=getattr(self, "available_models", None),
                    use_tools=False,
                )
                add = (getattr(resp, "text", None) or "").strip()
                if add:
                    if "append_file" in self.tool_manager.action_handlers:
                        await self.tool_manager.action_handlers["append_file"](file_path=path, content=("\n\n" if cur else "") + add)
                    elif "write_file" in self.tool_manager.action_handlers:
                        await self.tool_manager.action_handlers["write_file"](file_path=path, content=(cur + ("\n\n" if cur else "") + add))
                    # ê°±ì‹ 
                    try:
                        cur = _P(path).read_text(encoding="utf-8", errors="ignore")
                    except Exception:
                        return

            # --- ë¶„ëŸ‰ ë¶€ì¡±ì‹œ ì´ì–´ì“°ê¸°(ì¤‘ë³µ ë‹¨ë½ ì œê±° í¬í•¨) ---
            rounds = 0
            while len(cur) < min_chars and rounds < 2:
                from google.genai import types
                prompt = (
                    "ì•„ë˜ ë³¸ë¬¸ì˜ 'ë‹¤ìŒ ë¶€ë¶„'ì„ ìì—°ìŠ¤ëŸ½ê²Œ ì´ì–´ì„œ 'ë³¸ë¬¸ë§Œ' ì‘ì„±í•˜ì„¸ìš”.\n"
                    "- ê¸°ì¡´ ë¬¸ì¥/ë¬¸ë‹¨ ë°˜ë³µ ê¸ˆì§€\n"
                    "- ì‚¬ê±´/ë””í…Œì¼/ê°ì • ë¬˜ì‚¬ë¡œ ì „ê°œ í™•ì¥\n\n"
                    "[ì´ì „ ë³¸ë¬¸ (ë§ë¯¸ 4000ì)]\n" + cur[-4000:]
                )
                resp, _ = await self._call_llm_safe(
                    [types.Content(role="user", parts=[types.Part(text=prompt)])],
                    available_models=getattr(self, "available_models", None),
                    use_tools=False,
                )
                add = (getattr(resp, "text", None) or "").strip()
                if not add:
                    break
                add = self._dedupe_paragraphs(cur, add) if hasattr(self, "_dedupe_paragraphs") else add
                if not add:
                    break

                if "append_file" in self.tool_manager.action_handlers:
                    await self.tool_manager.action_handlers["append_file"](file_path=path, content="\n\n" + add)
                elif "write_file" in self.tool_manager.action_handlers:
                    await self.tool_manager.action_handlers["write_file"](file_path=path, content=(cur + "\n\n" + add))
                else:
                    break

                try:
                    cur = Path(path).read_text(encoding="utf-8", errors="ignore")
                except Exception:
                    break
                rounds += 1
        except Exception as _e:
            print(f"[PostWrite] continuation error: {str(_e)}")
    def _safe_append_text(self, path: str, text: str) -> None:
        from pathlib import Path as _P
        _P(path).parent.mkdir(parents=True, exist_ok=True)
        with open(path, "a", encoding="utf-8") as f:
            if text and not text.startswith("\n"):
                f.write("\n\n")
            f.write(text)
    async def _gen_initial_longform(self) -> str:
        from metatools.tool_selector import estimate_length_hint
        min_chars, target_chars = estimate_length_hint(self.user_goal or "", "longform_text")
        try:
            from google.genai import types
            prompt = (
                "ì•„ë˜ ì‚¬ìš©ì ëª©í‘œë¥¼ ì¶©ì‹¤íˆ ë°˜ì˜í•œ í•œêµ­ì–´ ì¥ë¬¸ ë³¸ë¬¸ì„ ë°”ë¡œ ì¶œë ¥í•˜ì„¸ìš”.\n"
                "- ë©”íƒ€ ì½”ë©˜íŠ¸/ì§€ì‹œì–´/ì‚¬ê³¼ë¬¸ ê¸ˆì§€, ë³¸ë¬¸ë§Œ ì¶œë ¥\n"
                f"- ìµœì†Œ {min_chars}ì ì´ìƒ, ê°€ëŠ¥í•˜ë©´ {max(min_chars+800, target_chars)}ì ë‚´ì™¸\n\n"
                "[ì‚¬ìš©ì ëª©í‘œ]\n" + (self.user_goal or "")
            )
            messages = [types.Content(role="user", parts=[types.Part(text=prompt)])]
        except Exception:
            messages = [{"role": "user", "parts": [{"text": self.user_goal or ""}]}]
        resp, _ = await self._call_llm_safe(messages, available_models=getattr(self, "available_models", None), use_tools=False)
        return (getattr(resp, "text", None) or "").strip()
    async def _post_write_boost_effect(self, effect: _Effect) -> None:
        try:
            cls = getattr(self, "_goal_classification", {}) or {}
            if cls.get("content_kind") != "longform_text":
                return
            path = effect.primary_path
            if not path:
                return

            from pathlib import Path as _P
            try:
                cur = _P(path).read_text(encoding="utf-8", errors="ignore")
            except Exception:
                cur = ""

            # ë¹„ì—ˆê±°ë‚˜ í”Œë ˆì´ìŠ¤í™€ë”ë§Œ ìˆìœ¼ë©´: ì²˜ìŒë¶€í„° ì±„ì›€ (ë©”íƒ€ ì½”ë©˜íŠ¸ ë°©ì§€)
            if (not cur.strip()) or self._is_auto_placeholder(cur):
                seed = await self._gen_initial_longform()
                if not seed:
                    return
                # overwrite (append_file ì—†ì–´ë„ ì•ˆì „)
                if "write_file" in self.tool_manager.action_handlers:
                    await self.tool_manager.action_handlers["write_file"](file_path=path, content=seed.strip())
                else:
                    _P(path).write_text(seed.strip(), encoding="utf-8")
                return

            # ë‚´ìš©ì´ ìˆì§€ë§Œ ë¶„ëŸ‰ ë¶€ì¡±í•˜ë©´ ì´ì–´ì“°ê¸°
            from metatools.tool_selector import estimate_length_hint
            min_chars, _ = estimate_length_hint(self.user_goal or "", "longform_text")
            rounds = 0
            while len(cur) < min_chars and rounds < 2:
                try:
                    from google.genai import types
                    prompt = (
                        "ì•„ë˜ í•œêµ­ì–´ ì¥ë¬¸ í…ìŠ¤íŠ¸ì˜ 'ë‹¤ìŒ ë¶€ë¶„'ì„ ìì—°ìŠ¤ëŸ½ê²Œ ì´ì–´ì„œ ì‘ì„±í•˜ì„¸ìš”.\n"
                        "- ê¸°ì¡´ ë¬¸ì¥/ë¬¸ë‹¨ ë°˜ë³µ ê¸ˆì§€, ë©”íƒ€ ì½”ë©˜íŠ¸ ê¸ˆì§€\n"
                        "- ì‚¬ê±´/ê°ì •/ë””í…Œì¼ ë¬˜ì‚¬ ì¶”ê°€ë¡œ ë¶„ëŸ‰ í™•ì¥\n\n"
                        "[ì´ì „ ë³¸ë¬¸ (ìµœê·¼ ë§¥ë½)]\n" + cur[-4000:]
                    )
                    resp, _ = await self._call_llm_safe(
                        [types.Content(role="user", parts=[types.Part(text=prompt)])],
                        available_models=self.available_models,
                        use_tools=False
                    )
                    add = (getattr(resp, "text", None) or "").strip()
                except Exception:
                    add = ""
                if not add:
                    break
                if hasattr(self, "_dedupe_paragraphs"):
                    add = self._dedupe_paragraphs(cur, add) or add
                if not add.strip():
                    break

                # append ìš°ì„ , ì—†ìœ¼ë©´ ì•ˆì „ ì¬ì“°ê¸°
                if "append_file" in self.tool_manager.action_handlers:
                    await self.tool_manager.action_handlers["append_file"](file_path=path, content="\n\n" + add)
                elif "write_file" in self.tool_manager.action_handlers:
                    await self.tool_manager.action_handlers["write_file"](file_path=path, content=cur + "\n\n" + add)
                else:
                    self._safe_append_text(path, add)

                try:
                    cur = _P(path).read_text(encoding="utf-8", errors="ignore")
                except Exception:
                    break
                rounds += 1
        except Exception as _e:
            print(f"[PostWrite] continuation error: {str(_e)}")
    async def _pivot_duplicate_backup(self, action: str, action_input: dict) -> tuple[str, dict]:
        # ë°±ì—…ì´ ì•„ë‹ˆë©´ ê·¸ëŒ€ë¡œ
        if action != "backup_file":
            return action, action_input
        from pathlib import Path as _P
        tgt = (action_input or {}).get("file_path") or (action_input or {}).get("src")
        if not tgt:
            return action, action_input
        norm = str(_P(tgt).resolve())

        # ì´ë¯¸ ë°±ì—…í–ˆê³  ì•„ì§ 'ì“°ê¸°'ê°€ ì—†ì—ˆë‹¤ë©´ â†’ ê³§ì¥ ì“°ê¸° ê³„ì—´ë¡œ ì „í™˜
        if norm in getattr(self, "_backed_up_paths", set()) and norm not in getattr(self, "_written_paths", set()):
            # ìš°ì„  append ì§€ì› ì‹œ append, ì•„ë‹ˆë©´ write
            if "append_file" in self.tool_manager.action_handlers:
                return "append_file", {"file_path": norm, "content": "<AUTO_seed>"}
            if "write_file" in self.tool_manager.action_handlers:
                return "write_file", {"file_path": norm, "content": "<AUTO_seed>"}
        return action, action_input
    import os, re
    async def _post_write_boost_effect(self, effect: _Effect) -> None:
        try:
            cls = getattr(self, "_goal_classification", {}) or {}
            if cls.get("content_kind") != "longform_text":
                return
            path = effect.primary_path
            if not path:
                return

            try:
                cur = _P(path).read_text(encoding="utf-8", errors="ignore")
            except Exception:
                cur = ""

            # í”Œë ˆì´ìŠ¤í™€ë”ë§Œ ë“¤ì–´ê°„ ê²½ìš° ë¹„ì–´ìˆëŠ” ë³¸ë¬¸ìœ¼ë¡œ ê°„ì£¼
            if self._is_auto_placeholder(cur):
                cur = ""

            from metatools.tool_selector import estimate_length_hint
            min_chars, _ = estimate_length_hint(self.user_goal or "", "longform_text")

            rounds = 0
            while len(cur) < min_chars and rounds < 2:
                try:
                    from google.genai import types
                    prompt = (
                        "ì´ì „ ë³¸ë¬¸ì˜ ë‹¤ìŒ ë¶€ë¶„ì„ ìì—°ìŠ¤ëŸ½ê²Œ ì´ì–´ì„œ ì‘ì„±í•˜ì„¸ìš”.\n"
                        "ì¤‘ë³µ ì—†ì´ ìƒˆë¡œìš´ ë‚´ìš©ì„ ì¶”ê°€í•˜ì„¸ìš”.\n"
                        "íŒŒì¼ì— ë°”ë¡œ ì´ì–´ë¶™ì¼ ë³¸ë¬¸ë§Œ ì¶œë ¥í•˜ì„¸ìš”.\n\n"
                        "[ì´ì „ ë³¸ë¬¸ (ìµœê·¼ ë§¥ë½)]\n" + cur[-4000:]
                    )
                    resp, _ = await self._call_llm_safe(
                        [types.Content(role="user", parts=[types.Part(text=prompt)])],
                        available_models=self.available_models,
                        use_tools=False
                    )
                    add = (getattr(resp, "text", None) or "").strip()
                except Exception:
                    add = ""

                if not add:
                    break
                if hasattr(self, "_dedupe_paragraphs"):
                    add = self._dedupe_paragraphs(cur, add) or add
                if not add.strip():
                    break

                self._safe_append_text(path, add)

                try:
                    cur = _P(path).read_text(encoding="utf-8", errors="ignore")
                except Exception:
                    break
                rounds += 1
        except Exception as _e:
            print(f"[PostWrite] continuation error: {str(_e)}")
    async def _pivot_duplicate_backup(self, action: str, action_input: dict) -> tuple[str, dict]:
        # longformì´ ì•„ë‹ˆë©´ íŒ¨ìŠ¤
        if not self._is_longform_goal():
            return action, action_input
        # ì´ë²ˆ ë°±ì—… ëŒ€ìƒ ê²½ë¡œ í™•ì¸
        from pathlib import Path as _P
        tgt = (action_input or {}).get("file_path") or (action_input or {}).get("src")
        if not tgt:
            return action, action_input
        norm = str(_P(tgt).resolve())

        # ì´ë¯¸ ë°±ì—…í–ˆê³ , ê·¸ ì´í›„ ì•„ì§ 'ì“°ê¸°'ê°€ ë°œìƒí•˜ì§€ ì•Šì•˜ë‹¤ë©´ â†’ write-likeë¡œ í”¼ë²—
        if norm in getattr(self, "_backed_up_paths", set()) and norm not in getattr(self, "_written_paths", set()):
            # ìš°ì„ ìˆœìœ„: append_file > write_file > ê·¸ ì™¸ write-like
            preferred = None
            for cand in ("append_file", "write_file"):
                if cand in self.tool_manager.action_handlers:
                    preferred = cand; break
            if not preferred:
                # ì¹´íƒˆë¡œê·¸ì—ì„œ write-like íƒìƒ‰(ë„êµ¬ëª… íŒ¨í„´)
                for d in getattr(self.tool_manager, "all_definitions", []):
                    nm = getattr(d, "name", "")
            if not preferred:
                return action, action_input  # ëŒ€ì²´ ë¶ˆê°€ ì‹œ ì›ë˜ëŒ€ë¡œ

            # íŒŒë¼ë¯¸í„° ì œì•ˆ (ìŠ¤í‚¤ë§ˆ ë§ì¶¤), íŒŒì¼ ê²½ë¡œëŠ” ë³´ì¡´
            from metatools.tool_selector import propose_parameters
            params = await propose_parameters(
                self.tool_manager,
                user_goal=self.user_goal or "",
                tool_name=preferred,
                available_models=getattr(self, "available_models", None),
                classification=getattr(self, "_goal_classification", None),
            ) or {}
            # ìŠ¤í‚¤ë§ˆì— file_pathê°€ ìˆë‹¤ë©´ ëˆ„ë½ ì‹œ ì±„ì›Œì¤Œ
            try:
                schema = self.tool_definitions.get(preferred, {})
                allowed = schema.get("allowed", set())
                if "file_path" in allowed and "file_path" not in params:
                    params["file_path"] = tgt
            except Exception:
                pass

            print(f"[BackupThrottle] duplicate backup detected â†’ pivot to '{preferred}'")
            return preferred, params

        return action, action_input
    async def _post_write_boost(self, tool_name: str, action_input: dict, observation: dict) -> None:
        """
        íŒŒì¼ì— ë¬´ì—‡ì¸ê°€ ì¨ì§„ ì§í›„, ëª©í‘œê°€ ì¥ë¬¸ì´ë©´ ë¶€ì¡±ë¶„ì„ appendë¡œ ë³´ê°•.
        - ë„êµ¬ëª…/í‚¤ì›Œë“œì— ì˜ì¡´í•˜ì§€ ì•Šê³ , ê´€ì¸¡ì¹˜/ì¸ìì—ì„œ 'ì–´ëŠ íŒŒì¼'ì¸ì§€ë§Œ ì¶”ì¶œ.
        - í˜„ì¬ íŒŒì¼ì´ ì‚¬ì‹¤ìƒ ë¹„ì—ˆìœ¼ë©´(í”Œë ˆì´ìŠ¤í™€ë” í¬í•¨) 'ì²˜ìŒë¶€í„°' ë³¸ë¬¸ì„ ìƒì„±í•´ ì±„ì›€.
        """
        try:
            cls = getattr(self, "_goal_classification", {}) or {}
            if cls.get("content_kind") != "longform_text":
                return

            # ê²½ë¡œ ì¼ë°˜í™” ì¶”ì¶œ (ë„êµ¬ëª… í•˜ë“œì½”ë”© ì—†ì´)
            path = self._extract_written_file_path(tool_name, action_input or {}, observation or {}) \
                or getattr(self, "last_written_file", "")
            if not path:
                return

            from pathlib import Path as _P
            try:
                cur = _P(path).read_text(encoding="utf-8", errors="ignore")
            except Exception:
                cur = ""

            # í˜„ì¬ê°€ 'ì‚¬ì‹¤ìƒ ë¹„ì–´ìˆìŒ' íŒë‹¨: ê³µë°±/í”Œë ˆì´ìŠ¤í™€ë”ë§Œ
            def _only_placeholder_or_empty(s: str) -> bool:
                if not s.strip():
                    return True
                ss = s.strip()
                return ss.startswith("<AUTO") and ss.endswith(">")

            from metatools.tool_selector import estimate_length_hint
            min_chars, _ = estimate_length_hint(self.user_goal or "", "longform_text")

            # 3-1) ë¹„ì—ˆìœ¼ë©´ ì²˜ìŒë¶€í„° ì±„ì›€
            if _only_placeholder_or_empty(cur):
                seed = await self._gen_initial_longform()
                if not seed:
                    return
                new_text = seed.strip()
                if "append_file" in self.tool_manager.action_handlers and cur.strip():
                    await self.tool_manager.action_handlers["append_file"](file_path=path, content="\n\n" + new_text)
                elif "write_file" in self.tool_manager.action_handlers:
                    await self.tool_manager.action_handlers["write_file"](file_path=path, content=new_text)
                return

            # 3-2) ë‚´ìš©ì´ ìˆìœ¼ë‚˜ ë¶„ëŸ‰ì´ ë¶€ì¡±í•˜ë©´ ì´ì–´ì“°ê¸°
            rounds = 0
            while len(cur) < min_chars and rounds < 2:
                try:
                    from google.genai import types
                    prompt = (
                        "ì•„ë˜ í•œêµ­ì–´ ì¥ë¬¸ í…ìŠ¤íŠ¸ì˜ 'ë‹¤ìŒ ë¶€ë¶„'ì„ ìì—°ìŠ¤ëŸ½ê²Œ ì´ì–´ì„œ ì‘ì„±í•˜ì„¸ìš”.\n"
                        "- ê¸°ì¡´ ë¬¸ì¥/ë¬¸ë‹¨ ë°˜ë³µ ê¸ˆì§€, ë©”íƒ€ ì½”ë©˜íŠ¸ ê¸ˆì§€\n"
                        "- ì‚¬ê±´/ê°ì •/ë””í…Œì¼ ë¬˜ì‚¬ ì¶”ê°€ë¡œ ë¶„ëŸ‰ í™•ì¥\n\n"
                        "[ì´ì „ ë³¸ë¬¸ (ìµœê·¼ ë§¥ë½)]\n" + cur[-4000:]
                    )
                    messages = [types.Content(role="user", parts=[types.Part(text=prompt)])]
                except Exception:
                    messages = [{"role": "user", "parts": [{"text": cur[-4000:]}]}]

                resp, _ = await self._call_llm_safe(messages, available_models=getattr(self, "available_models", None), use_tools=False)
                add = (getattr(resp, "text", None) or "").strip()
                if not add:
                    break

                # ì¤‘ë³µ ë¬¸ë‹¨ ì œê±° í›„ append
                add = self._dedupe_paragraphs(cur, add)
                if not add:
                    break

                if "append_file" in self.tool_manager.action_handlers:
                    await self.tool_manager.action_handlers["append_file"](file_path=path, content="\n\n" + add)
                elif "write_file" in self.tool_manager.action_handlers:
                    await self.tool_manager.action_handlers["write_file"](file_path=path, content=cur + "\n\n" + add)
                else:
                    break

                try:
                    cur = _P(path).read_text(encoding="utf-8", errors="ignore")
                except Exception:
                    break
                rounds += 1
        except Exception as _e:
            print(f"[PostWrite] continuation error: {str(_e)}")
    from typing import Optional, Tuple, Dict, Any
    def _auto_placeholder_keys(self, payload: dict) -> list:
        keys = []
        def _is_auto(x):
            return isinstance(x, str) and x.strip().startswith("<AUTO")
        for k, v in (payload or {}).items():
            if _is_auto(v):
                keys.append(k)
        return keys
    async def _decide_brevity_via_llm(self) -> bool:
        """
        ì‚¬ìš©ìê°€ 'ì§§ê²Œ/ê°„ë‹¨íˆ' ë“± ëª…ì‹œì ìœ¼ë¡œ ê°„ê²° ì‘ë‹µì„ ìš”ì²­í–ˆëŠ”ì§€ LLMì—ê²Œ ì˜ë¯¸ë¡œ íŒë‹¨ì‹œí‚¨ë‹¤.
        ë°˜í™˜: True(ê°„ê²° ìš”ì²­) | False(ê°„ê²° ìš”ì²­ ì•„ë‹˜)
        """
        user_goal = self.user_goal or ""
        instruction = (
            "Return JSON only: {\"brief\": true|false}.\n"
            "Output true ONLY IF the user explicitly asks for brevity (very short/one sentence/brief). "
            "If not explicit, return false."
        )
        prompt = f"{instruction}\n\nUSER_REQUEST:\n{user_goal}"
        try:
            text, _ = await self._call_llm_safe(self._build_user_message(prompt), use_tools=False)
            obj = {}
            try:
                # textëŠ” resp ê°ì²´ì¼ ìˆ˜ë„ ìˆê³  strì¼ ìˆ˜ë„ ìˆìŒ
                maybe = getattr(text, "text", None)
                obj = json.loads((maybe if isinstance(maybe, str) else (text if isinstance(text, str) else "")).strip())
            except Exception:
                obj = {}
            return bool(obj.get("brief", False))
        except Exception:
            return False
    def _pick_payload_field_fast(self, tool_spec: dict, action_input: dict) -> str | None:
        params = (tool_spec or {}).get("parameters", {}) or {}
        props  = params.get("properties") or {}
        if not isinstance(props, dict): return None
        keys = [k for k,v in props.items() if isinstance(v, dict) and v.get("type") == "string"]
        if not keys: return None

        def looks_path_or_url(s: str) -> bool:
            if not isinstance(s, str): return False
            s = s.strip()
            if s.startswith("http://") or s.startswith("https://"): return True
            # ì „ì²´ê°€ ê²½ë¡œ/í™•ì¥ìì— ê°€ê¹Œìš°ë©´ ì œì™¸
            import re
            if re.search(r"[\\/]", s) and re.search(r"\.[A-Za-z0-9]{1,5}($|\?)", s): return True
            return False

        # 1) í˜„ì¬ ê°’ ì¤‘ '<' í¬í•¨(HTML/ì½”ë“œ) ìš°ì„ 
        htmlish = [k for k in keys if isinstance(action_input.get(k), str) and "<" in action_input.get(k)]
        if htmlish:
            htmlish = [k for k in htmlish if not looks_path_or_url(action_input.get(k))]
            if htmlish:
                return htmlish[0]

        # 2) ê²½ë¡œ/URL ì œì™¸ í›„ ê°€ì¥ ê¸´ ë¬¸ìì—´
        candidates = []
        for k in keys:
            val = action_input.get(k, "")
            if looks_path_or_url(val): continue
            L = len(val) if isinstance(val, str) else -1
            candidates.append((L, k))
        if not candidates:
            return keys[0]
        candidates.sort(reverse=True)
        return candidates[0][1]
    async def _compose_or_enrich(self, user_goal: str, current_text: str, *, min_chars: int) -> str:
        if not isinstance(current_text, str) or len(current_text.strip()) < 200:
            # from-scratch
            prompt = (
                "ì‚¬ìš©ì ìš”ì²­ì„ ë°”íƒ•ìœ¼ë¡œ ì™„ì„± ë¬¸ì„œë¥¼ ì‘ì„±í•˜ì„¸ìš”.\n"
                "- í˜•ì‹ì„ ì ì ˆíˆ ì„ íƒ(ìš”ì²­ì´ HTML/ì½”ë“œ/ë¬¸ì„œ ë‰˜ì•™ìŠ¤ë©´ ê·¸ í˜•ì‹ìœ¼ë¡œ)\n"
                "- ë©”íƒ€ ë©˜íŠ¸ ê¸ˆì§€, ê²°ê³¼ë¬¼ë§Œ\n"
                f"- ë¶„ëŸ‰: ìµœì†Œ {min_chars}ì\n\n[ì‚¬ìš©ì ìš”ì²­]\n{user_goal}\n"
            )
        else:
            # preserve-format enrich
            prompt = (
                "CURRENT_TEXTë¥¼ í˜•ì‹ì„ ìœ ì§€í•˜ë©° ëŒ€í­ í™•ì¥í•˜ì„¸ìš”.\n"
                "- HTMLì´ë©´ ìœ íš¨í•œ HTML ìœ ì§€, ì½”ë“œë©´ ì½”ë“œë§Œ\n"
                "- ë©”íƒ€ ë©˜íŠ¸ ê¸ˆì§€\n"
                f"- ë¶„ëŸ‰: ìµœì†Œ {min_chars}ì\n\n[USER_GOAL]\n{user_goal}\n\n[CURRENT_TEXT]\n{current_text}\n"
            )
        resp, _ = await self._call_llm_safe(self._build_user_message(prompt), use_tools=False, preferred_model="gemini-2.5-flash")
        text = getattr(resp, "text", None) if not isinstance(resp, str) else resp
        return (text or "").strip()
    async def _enrich_payload_via_llm(self, *, user_goal: str, current_text: str, min_chars: int = 800) -> str:
        """
        í˜•ì‹ ë¶ˆë¬¸(HTML/Markdown/ì½”ë“œ/ìˆœìˆ˜ë¬¸ì„œ)ìœ¼ë¡œ 'í˜„ì¬ í…ìŠ¤íŠ¸'ë¥¼ ë” ê¸¸ê³  ì¶©ì‹¤í•˜ê²Œ í™•ì¥í•œë‹¤.
        - í˜„ì¬ í…ìŠ¤íŠ¸ì˜ í˜•ì‹ì„ ìœ ì§€(HTMLì´ë©´ HTML, ì½”ë“œë©´ ì½”ë“œ, MDë©´ MD, í‰ë¬¸ì´ë©´ ì¥ë¬¸ ê¸€)
        - ë©”íƒ€ ë©˜íŠ¸ ê¸ˆì§€, ê²°ê³¼ë¬¼ë§Œ ì¶œë ¥
        """
        # LLMì—ê²Œ 'í˜•ì‹ ìœ ì§€ + í™•ì¥'ì„ ê°•í•˜ê²Œ ì§€ì‹œ
        instruction = (
            "Expand and enrich the CURRENT_TEXT while preserving its exact format/type.\n"
            "- If it's HTML, keep valid, complete HTML (doctype optional if already present).\n"
            "- If it's code, output only the code (same language), no extra commentary.\n"
            "- If it's Markdown, keep Markdown.\n"
            "- If it's plain text, produce a long-form article in the same language.\n"
            "- Do not add explanations or meta text; output only the enriched content.\n"
            f"- Target length: at least {min_chars} characters.\n"
        )
        prompt = (
            f"{instruction}\n\n"
            f"[USER_GOAL]\n{user_goal}\n\n"
            f"[CURRENT_TEXT]\n{current_text or ''}\n"
        )
        resp, _ = await self._call_llm_safe(self._build_user_message(prompt), use_tools=False)
        text = ""
        try:
            text = getattr(resp, "text", None) or ""
        except Exception:
            pass
        if not text and isinstance(resp, str):
            text = resp
        return (text or "").strip()

    async def _maybe_enrich_short_text_payload(self, action: str, action_input: dict, *, min_chars: int = 800, min_threshold: int = 200) -> dict:
        """
        ë„êµ¬ ì‹¤í–‰ ì§ì „: í…ìŠ¤íŠ¸ í˜ì´ë¡œë“œê°€ ë„ˆë¬´ ì§§ìœ¼ë©´ ìë™ ë³´ê°•.
        - ê°„ê²° ìš”ì²­ì´ ëª…ì‹œëœ ê²½ìš°ëŠ” ìŠ¤í‚µ
        - ë„êµ¬ ìŠ¤í‚¤ë§ˆì—ì„œ string íŒŒë¼ë¯¸í„°ë¥¼ ì°¾ì•„ ê·¸ ì¤‘ í•˜ë‚˜ë¥¼ 'ì£¼ìš” í˜ì´ë¡œë“œ'ë¡œ ì„ íƒ
        - í˜„ì¬ ê¸¸ì´ê°€ min_threshold ë¯¸ë§Œì´ë©´ LLMìœ¼ë¡œ ë³´ê°•í•˜ì—¬ ì¹˜í™˜
        """
        try:
            # ê°„ê²° ìš”ì²­ì´ë©´ ê±´ë“œë¦¬ì§€ ì•ŠìŒ
            if await self._decide_brevity_via_llm():
                return action_input

            tool_spec = self.tool_manager.tool_catalog.get(action, {}) if hasattr(self.tool_manager, "tool_catalog") else {}
            key = self._pick_payload_field_fast(tool_spec, action_input or {})
            if not key:
                return action_input

            cur = action_input.get(key, "")
            if not isinstance(cur, str):
                return action_input

            if len(cur.strip()) >= min_threshold:
                return action_input

            enriched = await self._enrich_payload_via_llm(user_goal=self.user_goal or "", current_text=cur, min_chars=min_chars)
            if enriched and len(enriched) > len(cur):
                new_args = dict(action_input or {})
                new_args[key] = enriched
                return new_args
            return action_input
        except Exception:
            return action_input
    async def _call_llm_safe(self, messages, **kwargs):
        """
        - call_gemini_agent()ì— ì•ˆì „ ë˜í•‘
        - 429(RESOURCE_EXHAUSTED) ì‹œ RetryInfo.retryDelay ë§Œí¼ ëŒ€ê¸° í›„ ë™ì¼ ëª¨ë¸ 1íšŒ ì¬ì‹œë„
        ê·¸ë˜ë„ ì‹¤íŒ¨í•˜ë©´ available_modelsë¥¼ íšŒì „ì‹œì¼œ í´ë°± ì‹œë„
        - ë™ì¼ ì´í„°ë ˆì´ì…˜ ë‚´ ë™ì¼ í”„ë¡¬í”„íŠ¸ëŠ” ìºì‹œë¡œ ì¤‘ë³µ í˜¸ì¶œ ì°¨ë‹¨
        """
        import asyncio, random
        from inspect import signature

        # âœ… dedup í™œì„± ì—¬ë¶€ (ê¸°ë³¸ False)
        dedup = bool(kwargs.pop("dedup", False))

        # ì´í„°ë ˆì´ì…˜ ìºì‹œ ì¤€ë¹„
        cache = getattr(self, "_iter_prompt_cache", None)
        if not isinstance(cache, dict):
            self._iter_prompt_cache = {}
            cache = self._iter_prompt_cache

        # í”„ë¡¬í”„íŠ¸ ì¤‘ë³µ ì°¨ë‹¨: ë©”ì‹œì§€ í•´ì‹œ
        try:
            fp = self._messages_fingerprint(messages)
        except Exception:
            fp = None

        # âœ… dedupì´ Trueì¼ ë•Œë§Œ ìºì‹œ í™•ì¸
        if dedup and fp and fp in cache:
            cached = cache[fp]
            print("[LLM] dedup: return cached response for identical prompt in this iteration")
            return cached["response"], cached["model"]
        # ì‹œê·¸ë‹ˆì²˜ í•„í„°ë§(í”„ë¡œì íŠ¸ì˜ call_gemini_agentì— ë§ì¶° kwargs ì •ë¦¬)
        try:
            sig = signature(call_gemini_agent)  # ê¸°ì¡´ í”„ë¡œì íŠ¸ í•¨ìˆ˜
            filtered = {k: v for k, v in kwargs.items() if k in sig.parameters}
        except Exception:
            filtered = dict(kwargs)

        # ëª¨ë¸ ë¦¬ìŠ¤íŠ¸ í™•ë³´(ì—†ìœ¼ë©´ ë¹ˆ ë¦¬ìŠ¤íŠ¸)
        models = list(filtered.get("available_models") or getattr(self, "available_models", []) or [])

        async def _attempt(call_kwargs):
            # ì‹¤ì œ í˜¸ì¶œ: ì‹œê·¸ë‹ˆì²˜ì— ë§ì¶˜ kwargsë§Œ ì „ë‹¬
            return await call_gemini_agent(messages, **call_kwargs)
        # í˜¸ì¶œ ì„±ê³µ ì‹œ: âœ… dedup=Trueì¼ ë•Œë§Œ ìºì‹œ ì €ì¥
        try:
            if dedup and fp:
                cache[fp] = {"response": response, "model": model}
        except Exception:
            pass
        # 1ì°¨ ì‹œë„
        try:
            response, model = await _attempt(filtered)
            if fp:
                cache[fp] = {"response": response, "model": model}
            return response, model
        except Exception as e:
            msg = str(e)
            is_quota = ("RESOURCE_EXHAUSTED" in msg) or ("429" in msg)

            if is_quota:
                # RetryInfo ëŒ€ê¸° + ì§€í„°
                delay = self._parse_retry_delay_sec(e) + random.uniform(0.2, 0.8)
                print(f"[LLM] Quota hit. Sleeping ~{delay:.1f}s per RetryInfo...")
                await asyncio.sleep(delay)

                # ê°™ì€ ëª¨ë¸ë¡œ 1íšŒ ì¬ì‹œë„
                try:
                    response, model = await _attempt(filtered)
                    if fp:
                        cache[fp] = {"response": response, "model": model}
                    return response, model
                except Exception:
                    # ë‹¤ìŒ ëª¨ë¸ë¡œ í´ë°± (available_models íšŒì „)
                    if len(models) >= 2:
                        rotated = models[1:] + models[:1]
                        filtered["available_models"] = rotated
                        try:
                            response, model = await _attempt(filtered)
                            # ìš°ì„ ìˆœìœ„ ê°±ì‹  í›…ì´ ìˆìœ¼ë©´ í˜¸ì¶œ (ì—†ìœ¼ë©´ ë¬´ì‹œ)
                            bump = getattr(self, "_bump_model_priority", None)
                            if callable(bump):
                                try:
                                    bump(model)
                                except Exception:
                                    pass
                            if fp:
                                cache[fp] = {"response": response, "model": model}
                            return response, model
                        except Exception:
                            pass
            # 429ê°€ ì•„ë‹ˆê±°ë‚˜ ëª¨ë“  ì¬ì‹œë„ ì‹¤íŒ¨ â†’ ì›ë˜ ì˜ˆì™¸ ì „íŒŒ
            raise
    def _get_current_user_text(self) -> str:
        try:
            if hasattr(self, "last_user_message") and self.last_user_message:
                return self.last_user_message
            if hasattr(self, "user_goal") and self.user_goal:
                return self.user_goal
            return ""
        except Exception:
            return ""
    def _save_state(self):
        """í˜„ì¬ ëª¨ë¸ ìš°ì„ ìˆœìœ„ë¥¼ íŒŒì¼ì— ì €ì¥í•©ë‹ˆë‹¤."""
        try:
            with open(AGENT_STATE_PATH, 'w', encoding='utf-8') as f:
                json.dump({"model_priority_list": self.available_models}, f, indent=2)
                print(f"ğŸ’¾ ëª¨ë¸ ìš°ì„ ìˆœìœ„ê°€ íŒŒì¼ì— ì €ì¥ë˜ì—ˆìŠµë‹ˆë‹¤: {self.available_models}")
        except Exception as e:
            print(f"âš ï¸ Agent ìƒíƒœ ì €ì¥ ì‹¤íŒ¨: {e}")        
    async def _initialize(self):
        """ì„¸ì…˜ í™•ì¸, DB ìŠ¤í‚¤ë§ˆ ë¡œë”©, ì´ˆê¸° í”„ë¡¬í”„íŠ¸ êµ¬ì„± ë“± ì´ˆê¸°í™” ì‘ì—…ì„ ìˆ˜í–‰í•©ë‹ˆë‹¤."""
        session_exists = self.db.query(Session).filter(Session.session_id == self.session_id).first()
        if not session_exists:
            default_user = self.db.query(User).filter(User.user_name == 'ì‹ ì¢…í˜').first()
            self.current_user_id = default_user.user_id if default_user else 1
            new_session = Session(session_id=self.session_id, owner_id=self.current_user_id)
            self.db.add(new_session); self.db.commit()
        else:
            self.current_user_id = session_exists.owner_id
        
        schema_response = await asyncio.to_thread(get_db_schema)
        if schema_response["status"] != "success":
            raise Exception(f"DB ìŠ¤í‚¤ë§ˆ ë¡œë”© ì‹¤íŒ¨: {schema_response.get('message')}")
        schema_for_prompt = json.dumps(schema_response.get("schema", {}), indent=2, ensure_ascii=False)
        
        initial_prompt_text = build_prompt(
            f"**Current DB Schema:**\n```json\n{schema_for_prompt}\n```\n\n"
            f"**Current Session ID:** `{self.session_id}`\n\n"
            f"**ì‚¬ìš©ì ìš”ì²­ (ìµœì¢… ëª©í‘œ):**\n\"{self.user_goal}\""
        )
        self.prompt_content.append(types.Content(role="user", parts=[types.Part(text=initial_prompt_text)]))
    async def _call_model_and_parse_with_retry(self, max_retries: int = 1):
        """LLMì„ í˜¸ì¶œí•˜ê³ , íŒŒì‹± ì‹¤íŒ¨ ì‹œ ë³´ì •(ìµœëŒ€ 3íšŒ) í›„ì—ë„ ì•ˆ ë˜ë©´ ì•ˆì „ JSONìœ¼ë¡œ ê·€ê²°."""
        retry_count = 0
        raw_text = ""

        while retry_count <= max_retries:
            response, successful_model = await call_gemini_agent(
                prompt_content=self.prompt_content,
                available_models=self.available_models,
                tools=self.tool_manager.tools
            )

            if not response:
                # ì•ˆì „ JSON ë°˜í™˜ (ìµœì†Œí•œ final_response ì‹¤í–‰)
                safe_json = {
                    "Thought": "ëª¨ë¸ í˜¸ì¶œì— ì‹¤íŒ¨í•˜ì—¬ ì•ˆì „ ì¢…ë£Œí•©ë‹ˆë‹¤.",
                    "Action": {
                        "tool_name": "final_response",
                        "parameters": {
                            "answer": "ì ì‹œ ì˜¤ë¥˜ê°€ ìˆì—ˆì–´ìš”. ë‹¤ì‹œ ì‹œë„í•´ ì£¼ì„¸ìš”. (ëª¨ë¸ í˜¸ì¶œ ì‹¤íŒ¨)"
                        }
                    }
                }
                return safe_json["Thought"], safe_json["Action"]["tool_name"], safe_json["Action"]["parameters"], None

            if successful_model and successful_model != self.available_models[0]:
                print(f"[ğŸ’¡ Model Priority Updated] '{successful_model}'ì„(ë¥¼) ìµœìš°ì„  ëª¨ë¸ë¡œ ë³€ê²½í•©ë‹ˆë‹¤.")
                try:
                    self.available_models.remove(successful_model)
                except ValueError:
                    pass
                self.available_models.insert(0, successful_model)
                self._save_state()

            candidate = response.candidates[0]
            parts = getattr(candidate.content, "parts", [])
            if not parts:
                # ì•ˆì „ JSON ë°˜í™˜
                safe_json = {
                    "Thought": "ëª¨ë¸ì´ ë¹ˆ ì‘ë‹µì„ ë°˜í™˜í•˜ì—¬ ì•ˆì „ ì¢…ë£Œí•©ë‹ˆë‹¤.",
                    "Action": {
                        "tool_name": "final_response",
                        "parameters": {
                            "answer": "ì ì‹œ ì˜¤ë¥˜ê°€ ìˆì—ˆì–´ìš”. ë‹¤ì‹œ ì‹œë„í•´ ì£¼ì„¸ìš”. (ë¹ˆ ì‘ë‹µ)"
                        }
                    }
                }
                return safe_json["Thought"], safe_json["Action"]["tool_name"], safe_json["Action"]["parameters"], None

            part = parts[0]
            function_call = getattr(part, "function_call", None)
            if function_call:
                action = getattr(function_call, "name", None)
                action_input = dict(getattr(function_call, "args", {}))
                thought = action_input.get('thought', 'Function Callì—ì„œ ìƒê° ì¶”ì¶œ')
                if action:
                    return thought, action, action_input, candidate

            raw_text = getattr(part, "text", "")
            if raw_text:
                # 1ì°¨: ì§ì ‘ íŒŒì‹±
                obj = self._coerce_json_from_text(raw_text)
                if isinstance(obj, dict) and isinstance(obj.get("Action"), dict):
                    thought = obj.get("Thought", "")
                    action = obj["Action"].get("tool_name")
                    action_input = obj["Action"].get("parameters", {}) or {}
                    if action:
                        return thought, action, action_input, candidate

                # 2ì°¨: JSON ë³´ì •(ìµœëŒ€ 3íšŒ)
                print("âš ï¸ ëª¨ë¸ì´ ìœ íš¨ JSONì„ ë‚´ë†“ì§€ ì•Šì•˜ìŠµë‹ˆë‹¤. ë³´ì • ë£¨í”„ë¥¼ ì‹œì‘í•©ë‹ˆë‹¤...")
                repaired = await self._repair_json_via_llm(raw_text, max_attempts=3)
                if isinstance(repaired, dict):
                    thought = repaired.get("Thought", "")
                    action = (repaired.get("Action") or {}).get("tool_name")
                    action_input = (repaired.get("Action") or {}).get("parameters", {}) or {}
                    if action:
                        return thought, action, action_input, candidate

            # 3ì°¨: ë°”ê¹¥ ì¬ì‹œë„(í”„ë¡¬í”„íŠ¸ ë³´ê°• í›„ 1íšŒ ì¬í˜¸ì¶œ)
            retry_count += 1
            if retry_count > max_retries:
                break

            print(f"âš ï¸ ëª¨ë¸ì´ JSONì„ ë°˜í™˜í•˜ì§€ ì•Šì•„ 5ì´ˆ í›„ ì¬ì‹œë„í•©ë‹ˆë‹¤... (ì‹œë„ {retry_count}/{max_retries})")
            await asyncio.sleep(5)

            def _as_content(obj):
                if isinstance(obj, types.Content):
                    return obj
                if isinstance(obj, str):
                    return types.Content(role="model", parts=[types.Part(text=obj)])
                txt = getattr(obj, "text", None)
                if isinstance(txt, str):
                    return types.Content(role="model", parts=[types.Part(text=txt)])
                content = getattr(obj, "content", None)
                if content and getattr(content, "parts", None):
                    return content
                return types.Content(role="model", parts=[types.Part(text=str(obj))])


            self.prompt_content.append(_as_content(candidate))
            self.prompt_content.append(
                types.Content(role="user", parts=[types.Part(text="ë°˜ë“œì‹œ ë‹¨ì¼ JSON ê°ì²´ë¡œë§Œ ì‘ë‹µí•˜ì„¸ìš”. {\"Thought\":\"...\",\"Action\":{...}}")])
            )

        # ìµœì¢… ì‹¤íŒ¨: ì•ˆì „ JSON í´ë°±
        safe_json = {
            "Thought": "ëª¨ë¸ JSON ìƒì„±ì´ ì—°ì†ìœ¼ë¡œ ì‹¤íŒ¨í–ˆìœ¼ë¯€ë¡œ ì•ˆì „ ì¢…ë£Œí•©ë‹ˆë‹¤.",
            "Action": {
                "tool_name": "final_response",
                "parameters": {
                    "answer": "ì ì‹œ ì˜¤ë¥˜ê°€ ìˆì—ˆì–´ìš”. í•œ ë²ˆë§Œ ë‹¤ì‹œ ì‹œë„í•´ ì£¼ì„¸ìš”. (JSON ë³µêµ¬ ë£¨í”„ ì‹¤íŒ¨)"
                }
            }
        }
        return safe_json["Thought"], safe_json["Action"]["tool_name"], safe_json["Action"]["parameters"], None
    async def _propose_first_step(self):
        """
        [ìµœì í™”] ì‚¬ìš©ì ëª©í‘œë¥¼ ë¶„ì„í•˜ì—¬ (1)ì˜ë„ ë¶„ë¥˜ì™€ (2)ë„êµ¬ ê³„íšì„ ë‹¨ì¼ LLM í˜¸ì¶œë¡œ ì²˜ë¦¬í•©ë‹ˆë‹¤.
        ê°„ë‹¨í•œ ëŒ€í™”ëŠ” ë„êµ¬ ì‚¬ìš© ì—†ì´ ë°”ë¡œ ë‹¤ìŒ ë‹¨ê³„ë¡œ ë„˜ì–´ê°‘ë‹ˆë‹¤.
        """
        user_goal_text = (self.user_goal or "").strip()
        print(f"[ğŸš€ Proposing First Step] í†µí•© ë¶„ì„ ë° ê³„íšì„ ì‹œì‘í•©ë‹ˆë‹¤: '{user_goal_text}'")

        try:
            # --- 1. ë‹¨ì¼ ë¶„ì„/ê³„íšì„ ìœ„í•œ í”„ë¡¬í”„íŠ¸ êµ¬ì„± ---
            _all_defs = [d for d in getattr(self.tool_manager, "all_definitions", []) if getattr(d, "name", "")]
            _BANNABLE = {"final_response", "ask_followup_question", "clarify", "reflect"}
            BANNED_TERMINALS = {d.name for d in _all_defs if d.name in _BANNABLE}
            _defs_no_terminals = [d for d in _all_defs if d.name not in BANNED_TERMINALS]

            def _dump_catalog(defs):
                # (ì´ í—¬í¼ í•¨ìˆ˜ëŠ” ê¸°ì¡´ê³¼ ë™ì¼í•˜ê²Œ ìœ ì§€)
                meta = getattr(self.tool_manager, "tool_meta", {})
                parts = []
                for d in defs:
                    name = getattr(d, "name", "")
                    desc = getattr(d, "description", "")
                    m = (meta.get(name, {}) or {})
                    tags = m.get("tags", [])
                    dep = m.get("dependency_graph", {}) or {}
                    nodes = dep.get("nodes") or []
                    covers = [n.get("tool_name") for n in nodes if isinstance(n, dict) and n.get("tool_name")]
                    lines = [f"- {name} : {desc}", f"  tags: {tags}"]
                    if covers:
                        lines.append(f"  covers: {', '.join(covers)}")
                    parts.append("\n".join(lines))
                return "\n".join(parts)

            tool_catalog_dump = _dump_catalog(_defs_no_terminals)

            # í†µí•© í”„ë¡¬í”„íŠ¸
            unified_prompt = (
                "ë‹¹ì‹ ì€ ì‚¬ìš©ìì˜ ìš”ì²­ì„ ë¶„ì„í•˜ê³  ì‹¤í–‰ ê³„íšì„ ì„¸ìš°ëŠ” 'ì§€ëŠ¥í˜• í”Œë˜ë„ˆ'ì…ë‹ˆë‹¤.\n"
                "ìš”ì²­ì„ ë¶„ì„í•˜ì—¬ (1)ì˜ë„ ë¶„ë¥˜ì™€ (2)ì‹¤í–‰ ê³„íšì„ í•˜ë‚˜ì˜ JSON ê°ì²´ë¡œ ì‘ë‹µí•˜ì„¸ìš”.\n\n"
                "[ì˜ë„ ë¶„ë¥˜ ê¸°ì¤€]\n"
                "- 'chat': ê°„ë‹¨í•œ ì¸ì‚¬, ê°ì • í‘œí˜„, ì§§ì€ ì§ˆë¬¸ ë“± ë„êµ¬ê°€ í•„ìš” ì—†ëŠ” ì¼ë°˜ ëŒ€í™”.\n"
                "- 'task': íŒŒì¼ ì‘ì—…, ì •ë³´ ì¡°íšŒ, ì½”ë“œ ìƒì„± ë“± ë„êµ¬ ì‚¬ìš©ì´ í•„ìš”í•œ ëª…í™•í•œ ì‘ì—….\n\n"
                "[ì‚¬ìš© ê°€ëŠ¥í•œ ë„êµ¬ ëª©ë¡]\n"
                f"{tool_catalog_dump}\n\n"
                "[ê·œì¹™]\n"
                "- ì˜ë„ê°€ 'chat'ì´ë©´, 'steps'ëŠ” ë¹ˆ ë°°ì—´ `[]`ì´ì–´ì•¼ í•©ë‹ˆë‹¤.\n"
                "- ì˜ë„ê°€ 'task'ì´ë©´, ëª©í‘œ ë‹¬ì„±ì— í•„ìš”í•œ ë„êµ¬ ëª©ë¡ì„ 'steps'ì— ìˆœì„œëŒ€ë¡œ ë‚˜ì—´í•˜ì„¸ìš”.\n"
                "- ê°€ëŠ¥í•œ ê²½ìš°, ì—¬ëŸ¬ ë‹¨ê³„ë¥¼ í¬ê´„í•˜ëŠ” ë§¤í¬ë¡œ ë„êµ¬ë¥¼ ìš°ì„ ì ìœ¼ë¡œ ì‚¬ìš©í•˜ì„¸ìš”.\n"
                "- ì¢…ë£Œ/ì§ˆë¬¸ ê´€ë ¨ ë„êµ¬(final_response ë“±)ëŠ” 'steps'ì— í¬í•¨í•˜ì§€ ë§ˆì„¸ìš”.\n"
                "- ì¶œë ¥ì€ ë°˜ë“œì‹œ ì•„ë˜ í˜•ì‹ì˜ ìˆœìˆ˜ JSON ê°ì²´ì—¬ì•¼ í•©ë‹ˆë‹¤.\n\n"
                "[ì‚¬ìš©ì ëª©í‘œ]\n"
                f'"{user_goal_text}"\n\n'
                "[ì¶œë ¥ í˜•ì‹ - JSON Only]\n"
                "{\n"
                '  "classification": {\n'
                '    "intent": "chat" | "task"\n'
                '  },\n'
                '  "thought": "ê³„íšì— ëŒ€í•œ ê°„ë‹¨í•œ ìƒê°.",\n'
                '  "steps": ["ë„êµ¬ëª…1", "ë„êµ¬ëª…2", ...]\n'
                "}"
            )

            # --- 2. ë‹¨ì¼ LLM í˜¸ì¶œ ---
            response, _ = await call_gemini_agent(
                [types.Content(role="user", parts=[types.Part(text=unified_prompt)])],
                available_models=self.available_models,
                use_tools=False
            )

            if (not response) or (not getattr(response, "text", None)):
                print("[ğŸ” Plan] 1ì°¨ ì‹¤íŒ¨ â†’ ë³´ìˆ˜ ì„¤ì •ìœ¼ë¡œ ì¬ì‹œë„í•©ë‹ˆë‹¤.")
                response, successful_model = await call_gemini_agent(
                    [types.Content(role="user", parts=[types.Part(text=unified_prompt)])],
                    available_models=self.available_models,
                    use_tools=False,
                    temperature=0  # llm_utils.call_gemini_agent ì‹œê·¸ë‹ˆì²˜ì— ì¡´ì¬
                )
                if (not response) or (not getattr(response, "text", None)):
                    print("[ğŸ” Plan] í†µí•© ë¶„ì„/ê³„íš ìƒì„± ì‹¤íŒ¨. ì¼ë°˜ ëª¨ë“œë¡œ ì§„í–‰í•©ë‹ˆë‹¤.")
                    # (ì„ íƒ) ì´í›„ ë¡œì§ ì¼ê´€ì„±ì„ ìœ„í•´ ë¹ˆ í”Œëœ ìºì‹œ
                    self._latest_plan_obj = {"steps": []}
                    return None, None

            # --- 3. í†µí•© ì‘ë‹µ íŒŒì‹± ë° ì²˜ë¦¬ ---
            raw_text = getattr(response, "text", "")
            plan_obj = self._coerce_json_from_text(raw_text)

            if not isinstance(plan_obj, dict):
                print(f"[ğŸ” Plan] ìœ íš¨í•œ JSON ì‘ë‹µ íŒŒì‹± ì‹¤íŒ¨. Raw: {raw_text}")
                self._latest_plan_obj = {"steps": []}
                return None, None

            # 3-1. ë¶„ë¥˜ ê²°ê³¼ ì €ì¥ ë° í™•ì¸
            classification = plan_obj.get("classification", {})
            self._goal_classification = classification # ì—ì´ì „íŠ¸ ìƒíƒœì— ì €ì¥
            intent = classification.get("intent")
            
            print(f"[Classifier] ëª¨ë¸ì´ ì˜ë„ë¥¼ '{intent}'ë¡œ íŒë‹¨í–ˆìŠµë‹ˆë‹¤.")

            # 3-2. ë„êµ¬ ì‹¤í–‰ ê³„íš ì¶”ì¶œ
            steps = plan_obj.get("steps")
            if not isinstance(steps, list):
                print("[ğŸ” Plan] 'steps'ê°€ ìœ íš¨í•œ ë¦¬ìŠ¤íŠ¸ê°€ ì•„ë‹™ë‹ˆë‹¤. ê³„íšì„ ë¹„ì›ë‹ˆë‹¤.")
                steps = []

            # 3-3. ê°„ë‹¨í•œ ëŒ€í™”(chat)ë¡œ ë¶„ë¥˜ë˜ë©´, ê³„íš ì—†ì´ ë°”ë¡œ ì¢…ë£Œ
            if intent == 'chat' or not steps:
                print("ğŸ’¬ ê°„ë‹¨í•œ ëŒ€í™”ë¡œ íŒë‹¨í•˜ì—¬, ë„êµ¬ ê³„íš ì—†ì´ ì¼ë°˜ ì‘ë‹µì„ ì§„í–‰í•©ë‹ˆë‹¤.")
                # Noneì„ ë°˜í™˜í•˜ì—¬ ë©”ì¸ ë£¨í”„ê°€ ì¼ë°˜ LLM í˜¸ì¶œì„ í•˜ë„ë¡ ìœ ë„
                return None, None
            
            # --- 4. (ê¸°ì¡´ ë¡œì§ê³¼ ìœ ì‚¬) ê³„íšì´ ìˆì„ ê²½ìš° ì²« ë‹¨ê³„ ì œì•ˆ ---
            # ì¹´íƒˆë¡œê·¸ì— ì¡´ì¬í•˜ëŠ” ìœ íš¨í•œ ë„êµ¬ë§Œ í•„í„°ë§
            catalog_set = {d.name for d in _all_defs}
            remaining_steps = [s for s in steps if isinstance(s, str) and s in catalog_set]

            if not remaining_steps:
                print("[ğŸ” Plan] ê³„íšëœ ë‹¨ê³„ê°€ ìˆì§€ë§Œ, ìœ íš¨í•œ ë„êµ¬ê°€ ì—†ìŠµë‹ˆë‹¤.")
                return None, None

            # (ì„ íƒì ) ê¸°ì¡´ì˜ set_cover ë¡œì§ì„ ì—¬ê¸°ì— ì ìš©í•˜ì—¬ ê³„íšì„ ë” ìµœì í™”í•  ìˆ˜ ìˆìŠµë‹ˆë‹¤.
            # remaining_steps = _apply_set_cover(remaining_steps)
            # âœ… ì—¬ê¸° ì¶”ê°€: ì •ê·œí™”ëœ ê³„íšì„ ì„¸ì…˜ ìƒíƒœë¡œ ì €ì¥ (ì´í›„ _get_planned_steps()ì—ì„œ ì‚¬ìš©)
            self._latest_plan_obj = {"steps": list(remaining_steps)}
            # ì²« ë²ˆì§¸ ë‹¨ê³„ë¥¼ ì œì•ˆ
            first_tool = remaining_steps[0]
            print(f"[âœ… First Step Proposed] AIê°€ ì²« ë‹¨ê³„ë¡œ '{first_tool}' ë„êµ¬ë¥¼ ì œì•ˆí–ˆìŠµë‹ˆë‹¤.")

            # íŒŒë¼ë¯¸í„° ìƒì„±
            params = await propose_parameters(
                self.tool_manager,
                user_goal=user_goal_text,
                tool_name=first_tool,
                available_models=self.available_models,
            )

            return first_tool, params

        except Exception as e:
            import traceback
            print(f"[First Step Fatal] ì²« ë‹¨ê³„ ì œì•ˆ ì¤‘ ì‹¬ê°í•œ ì˜¤ë¥˜ ë°œìƒ: {e}")
            traceback.print_exc()
            return None, None
    def _emit_thought(self, text: str) -> None:
        """
        Thought UI/ë””ë²„ê¹… ìŠ¤íŠ¸ë¦¼ìœ¼ë¡œ ë³´ëƒ…ë‹ˆë‹¤. event_busê°€ ì—†ìœ¼ë©´ ë¡œê·¸ë¡œ fallback.
        """
        try:
            bus = getattr(self, "event_bus", None)
            if bus:
                bus.emit("agent_thought", {"text": str(text)})
            else:
                print(f"[THOUGHT] {text}")
        except Exception as e:
            print(f"[Thought Emit Warning] {e}")
    def _sanitize_action_input_and_emit_thought(self, action_input: dict, top_level_json: dict | None = None) -> dict:
        """
        - top_level_jsonì˜ Thought(ë˜ëŠ” thought)ë¥¼ ì´ë²¤íŠ¸ë¡œ ë‚´ë³´ë‚´ê³ 
        - ë„êµ¬ íŒŒë¼ë¯¸í„°ì—ì„œ ë©”íƒ€í‚¤ë¥¼ ì œê±°í•©ë‹ˆë‹¤.
        """
        META_KEYS = {"Thought", "thought", "notes", "debug", "reason", "rationale"}
        # 1) Thought ì´ë²¤íŠ¸
        try:
            if isinstance(top_level_json, dict):
                t = top_level_json.get("Thought") or top_level_json.get("thought")
                if t:
                    self._emit_thought(t)
        except Exception as e:
            print(f"[Thought Extract Warning] {e}")

        # 2) META í‚¤ ì œê±°
        try:
            clean = {}
            for k, v in (action_input or {}).items():
                if k not in META_KEYS:
                    clean[k] = v
            return clean
        except Exception as e:
            print(f"[META Filter Warning] {e}")
            # ë¬¸ì œ ë°œìƒ ì‹œ ì›ë³¸ì„ ê·¸ëŒ€ë¡œ ë°˜í™˜
            return action_input or {}
    def _is_longform_goal(self) -> bool:
        """í˜„ì¬ ëª©í‘œê°€ ì¥ë¬¸ í…ìŠ¤íŠ¸ ìƒì„±ì¸ì§€ í™•ì¸(ë¶„ë¥˜ê¸° ê²°ê³¼ ì°¸ì¡°)."""
        cls = getattr(self, "_goal_classification", {}) or {}
        return cls.get("content_kind") == "longform_text"
    def _as_content(self, x):
        # ì´ë¯¸ ë”•ì…”ë„ˆë¦¬ í˜•íƒœì˜ contentë©´ ê·¸ëŒ€ë¡œ ë°˜í™˜ (í˜¸í™˜ì„±ì„ ìœ„í•´ ì¶”ê°€)
        if isinstance(x, dict) and "parts" in x and "role" in x:
            return 
        # ë¬¸ìì—´ì¸ì§€, .text ì†ì„±ì´ ìˆëŠ”ì§€ ë“±ì„ í™•ì¸í•˜ì—¬ í…ìŠ¤íŠ¸ ì¶”ì¶œ
        text_content = ""
        if isinstance(x, str):
            text_content = x
        elif hasattr(x, "text") and isinstance(getattr(x, "text"), str):
            text_content = getattr(x, "text")
        else:
            text_content = str(x)
        # ìµœì¢…ì ìœ¼ë¡œ ë”•ì…”ë„ˆë¦¬ í˜•íƒœë¡œ ë§Œë“¤ì–´ ë°˜í™˜
        return {"role": "model", "parts": [text_content]}
    def _calc_target_chars(self, cap_tokens=65536, reserve=12000, tok_to_char=1.6):
        usable = max(2048, int(cap_tokens * 0.6) - reserve)
        return int(usable * tok_to_char)
    async def _execute_tool(self, action: str, params: dict):
        """
        ëª¨ë“  ë„êµ¬ í˜¸ì¶œì€ ì´ í•¨ìˆ˜ë¡œë§Œ ë“¤ì–´ì˜¤ê²Œ í•œë‹¤.
        1) (í˜•ì‹ ë¶ˆë¬¸) í…ìŠ¤íŠ¸ í˜ì´ë¡œë“œ ìë™ ë³´ê°•(í‘œì¤€)
        2) ensure_args ì •ê·œí™”
        3) ì‹¤íŒ¨ ì‹œ ë³´ê°•(ê°•) 1íšŒ â†’ ì¬-ensure
        4) ìµœì¢…ì ìœ¼ë¡œ _call_tool ë¡œ ì‹¤í–‰ (ì¬ê·€ ê¸ˆì§€)
        """
        params = dict(params or {})
        tool_spec = getattr(self.tool_manager, "tool_catalog", {}).get(action, {}) if hasattr(self.tool_manager, "tool_catalog") else {}

        # 1) í‘œì¤€ ë³´ê°•(ê°„ê²° ìš”ì²­ ì•„ë‹ˆê³ , í…ìŠ¤íŠ¸ê°€ ì§§ìœ¼ë©´)
        if hasattr(self, "_maybe_enrich_text_payload_strong"):
            params = await self._maybe_enrich_text_payload_strong(
                action=action,
                action_input=params,
                tool_spec=tool_spec,
                min_threshold=5000,              # "ì§§ìŒ" íŒë‹¨ ê¸°ì¤€(ë¬¸ì)
                min_chars=self._calc_target_chars()  # ì¶œë ¥ ìƒí•œ ê³ ë ¤í•´ ë™ì ìœ¼ë¡œ ê³„ì‚°
            )

        # 2) ì •ê·œí™”
        ok, normed, err = self.tool_manager.ensure_args(action, params)
        if ok:
            return await self._call_tool(action, normed)

        # 3) ì‹¤íŒ¨ ì‹œ 'ê°• ë³´ê°•' 1íšŒ
        if hasattr(self, "_maybe_enrich_text_payload_strong"):
            params2 = await self._maybe_enrich_text_payload_strong(
                action=action,
                action_input=params,
                tool_spec=tool_spec,
                min_threshold=8000,                              # ë” ì—„ê²©
                min_chars=max(self._calc_target_chars(), 30000)  # ëª©í‘œ ê¸¸ì´ ìƒí–¥(ë¬¸ì)
            )
            ok2, normed2, err2 = self.tool_manager.ensure_args(action, params2)
            if ok2:
                return await self._call_tool(action, normed2)

        # 4) ìµœí›„ ìˆ˜ë‹¨: ìˆëŠ” ê·¸ëŒ€ë¡œ ì‹¤í–‰(ë˜ëŠ” ì—¬ê¸°ì„œ ì‹¤íŒ¨ ë°˜í™˜)
        return await self._call_tool(action, params)



    async def _run_iteration(self, iteration: int, first_step_proposal: tuple = (None, None)):
        """
        í•œ ì‚¬ì´í´ ì‹¤í–‰:
        - ì²« ì´í„°ë ˆì´ì…˜ì— ì œì•ˆëœ ë‹¨ê³„ê°€ ìˆìœ¼ë©´ ìš°ì„  ì‚¬ìš©
        - ëª¨ë¸ í˜¸ì¶œë¡œ action, action_input, thought ê²°ì •
        - ë„êµ¬ ì‹¤í–‰ì€ kwargsë¡œë§Œ í˜¸ì¶œ
        - (ì¡´ì¬ ì‹œ) generated_definitions ê¸°ë°˜ ìŠ¤í‚¤ë§ˆ ê²€ì¦ ìˆ˜í–‰
        - ì™„ë£Œ í›„ prompt_contentì— function_call/function_response ê¸°ë¡
        - ë°±ê·¸ë¼ìš´ë“œë¡œ optimization materializer íŠ¸ë¦¬ê±°
        - ì‹¤í–‰ ê²°ê³¼(ì„±ê³µ/ì‹¤íŒ¨, ì§€ì—°) ë¥¼ ToolRegistryì— reportí•˜ì—¬ ì´í›„ ì¶”ì²œ í’ˆì§ˆ í–¥ìƒ
        """
        import json
        print(f"\n======== Agent Iteration {iteration + 1} (Session: {self.session_id}) ========")

        thought, action, action_input, candidate = None, None, None, None
        observation_result = None  # â† ë¯¸ë¦¬ ì„ ì–¸í•´ UnboundLocalError ë°©ì§€

        # 1) ì²« ì´í„°ë ˆì´ì…˜: ì œì•ˆëœ ë‹¨ê³„ ìš°ì„  ì‚¬ìš©
        proposed_action, proposed_input = first_step_proposal
        if iteration == 0 and proposed_action:
            print(f"ğŸ’¡ ì œì•ˆëœ ì²« ë‹¨ê³„({proposed_action})ë¥¼ ì‚¬ìš©í•˜ì—¬ ì´í„°ë ˆì´ì…˜ì„ ì‹œì‘í•©ë‹ˆë‹¤.")
            action = proposed_action
            action_input = proposed_input or {}
            thought = action_input.get("thought", f"ì œì•ˆëœ ì²« ë‹¨ê³„ '{action}'ì„ ì‹¤í–‰í•©ë‹ˆë‹¤.")
            try:
                candidate_content_part = types.Part(function_call=types.FunctionCall(name=action, args=action_input))
                candidate = types.Candidate(content=types.Content(role="model", parts=[candidate_content_part]))
            except NameError:
                candidate = None
        else:
            # 2) ì´í›„ ì´í„°ë ˆì´ì…˜: ëª¨ë¸ í˜¸ì¶œ
            thought, action, action_input, candidate = await self._call_model_and_parse_with_retry()

        if not action:
            error_message = (action_input or {}).get("error", "ì•Œ ìˆ˜ ì—†ëŠ” íŒŒì‹± ì˜¤ë¥˜")
            # observation_resultëŠ” í•­ìƒ dictë¡œ! (í¬ë˜ì‹œ ë°©ì§€)
            return True, thought, "error", {"error": error_message}, {
                "status": "error",
                "reason": error_message
            }
        action_input = self._sanitize_action_input_and_emit_thought(
            action_input,
            {"Thought": thought} if thought else None
        )
        print(f"[âœ… Agent Response] Thought: {thought}")
        print(f"  Action: {action}, Input: {action_input}")

        # 3) ì›Œí¬í”Œë¡œìš° ê¸°ë¡(ë„êµ¬ í˜¸ì¶œ ì „) â€” thought ì œê±°
        if action != "final_response":
            try:
                params_for_workflow = dict(action_input or {})
                params_for_workflow.pop("thought", None)
                self.session_workflow.append(json5.dumps({"tool_name": action, "parameters": params_for_workflow}))
            except Exception as e:
                print(f"[Workflow Log Warning] {e}")
        # (í•¸ë“¤ëŸ¬ ì‹¤í–‰ ì´í›„ë¡œ ì´ë™í•œ PostWriteëŠ” ì•„ë˜ì—ì„œ í˜¸ì¶œ)
        if isinstance(observation_result, dict) and observation_result.get("status") == "ok":
            await self._post_write_boost(action, action_input or {}, observation_result or {})
            # ê¸¸ì´ ì¶©ì¡± ì‹œ ìë™ ì¢…ë£Œ(ëª¨ë¸ì´ final_responseë¥¼ ëª» ë‚´ë„ ë§ˆë¬´ë¦¬)
            try:
                cls = getattr(self, "_goal_classification", {}) or {}
                if cls.get("content_kind") == "longform_text":
                    obs = observation_result
                    det = obs.get("detail", {}) if isinstance(obs.get("detail"), dict) else {}
                    path = det.get("path") or obs.get("path") or getattr(self, "last_written_file", "")
                    if path:
                        from metatools.tool_selector import estimate_length_hint
                        min_chars, max_chars = estimate_length_hint(self.user_goal or "", "longform_text")
                        try:
                            from pathlib import Path as _P
                            cur = _P(path).read_text(encoding="utf-8", errors="ignore")
                        except Exception:
                            cur = ""
                        if len(cur) >= min_chars:
                            final_answer = f"íŒŒì¼ì— ê²°ê³¼ë¥¼ ì €ì¥í–ˆìŠµë‹ˆë‹¤: {path}"
                            return True, thought, action, {"answer": final_answer}, {"answer": final_answer}
            except Exception as _e:
                    print(f"[PostWrite] finalize check skipped: {_e}")
        # 4) ìµœì¢… ì‘ë‹µ ì²˜ë¦¬
        if action == "final_response":
            final_answer = (action_input or {}).get("answer", "")
            print(f"[ğŸ Final Answer] {final_answer}")

                # ëª¨ë¸ ê¸°ë¡
            try:
                if candidate:
                    self.prompt_content.append(self._as_content(candidate))
                else:
                    self.prompt_content.append(
                        types.Content(
                            role="model",
                            parts=[types.Part(function_call=types.FunctionCall(name=action, args=action_input or {}))]
                        )
                    )
                self.prompt_content.append(
                    types.Content(
                        role="user",  # function_responseëŠ” user ì—­í•  ë©”ì‹œì§€ì— ë‹´ëŠ” ê²Œ ê·œì•½
                        parts=[types.Part(function_response=types.FunctionResponse(
                            name=action,
                            response={"result": {"answer": final_answer}}
                        ))]
                    )
                )
            except NameError:
                pass

            # ë°±ê·¸ë¼ìš´ë“œ ìµœì í™” íŠ¸ë¦¬ê±°
            try:
                import asyncio
                if not hasattr(self, "optim_manager") or self.optim_manager is None:
                    from optimization_manager import OptimizationManager
                    self.optim_manager = OptimizationManager()
                generate_tool_definition_and_code = globals().get("generate_tool_definition_and_code")
                register_newly_generated_tool = globals().get("register_newly_generated_tool")
                handle_reload_skills = globals().get("handle_reload_skills")
                if generate_tool_definition_and_code and register_newly_generated_tool and handle_reload_skills:
                    workflow_str_list = getattr(self, "session_workflow", [])
                    user_goal_for_optimization = getattr(self, "user_goal", None)
                    asyncio.create_task(
                        self.optim_manager.record_and_maybe_materialize(
                            workflow_str_list,
                            user_goal_for_optimization,
                            generate_tool_definition_and_code=generate_tool_definition_and_code,
                            register_newly_generated_tool=register_newly_generated_tool,
                            handle_reload_skills=handle_reload_skills
                        )
                    )
            except Exception as e:
                print(f"[Optimization Hook Error] {e}")

            return True, thought, action, action_input, {"answer": final_answer}
        # âœ… ì§§ì€ í…ìŠ¤íŠ¸ ìë™ ë³´ê°•(í˜•ì‹ ë¶ˆë¬¸)
        action_input = await self._maybe_enrich_short_text_payload(action, action_input or {}, min_chars=800, min_threshold=200)

        # ì´í›„ ì •ê·œí™” â†’ ì‹¤ì œ ë„êµ¬ í˜¸ì¶œ
        ok, normed, err = self.tool_manager.ensure_args(action, action_input or {})
        # (ë„êµ¬ ì‹¤í–‰ ì§ì „) ë ˆì§€ìŠ¤íŠ¸ë¦¬ ìš°ì„  ì¹˜í™˜ ì‹œë„
        try:
            action, action_input, _override_meta = self._maybe_override_with_registry_tool(action, action_input)
            if _override_meta.get("overridden"):
                print(f"[Registry Override] {json.dumps(_override_meta, ensure_ascii=False)}")
        except Exception as e:
            print(f"[Registry Override Warning] {e}")
        # 2) í•¸ë“¤ëŸ¬ ì‹¤í–‰ 'ì§ì „'ì— ë„£ì„ ê°€ë“œ: í”Œë ˆì´ìŠ¤í™€ë”ë¡œëŠ” ì ˆëŒ€ ì‹¤í–‰í•˜ì§€ ì•Šê¸°
        # ìœ„ì¹˜: _run_iteration ë‚´ë¶€, ì‹¤ì œ handler í˜¸ì¶œ ì§ì „ì— ì‚½ì…
        if isinstance(action_input, dict):
            # longform ëª©í‘œì´ê³ , ì…ë ¥ì— <AUTO...> í”Œë ˆì´ìŠ¤í™€ë”ê°€ í•˜ë‚˜ë¼ë„ ìˆìœ¼ë©´ ëŒ€ì²´
            cls = getattr(self, "_goal_classification", {}) or {}
            if cls.get("content_kind") == "longform_text":
                keys = self._auto_placeholder_keys(action_input)
                if keys:
                    seed = await self._gen_initial_longform()
                    for k in keys:
                        action_input[k] = seed
        # ğŸ”’ ì¤‘ë³µ ë°±ì—… â†’ ì“°ê¸°ë¥˜ë¡œ ìë™ í”¼ë²—
        try:
            action, action_input = await self._pivot_duplicate_backup(action, action_input)
        except Exception as e:
            print(f"[BackupThrottle Warning] {e}")
        action, action_input, meta = self._maybe_override_with_registry_tool(action, action_input or {})
        if meta.get("overridden"):
            print(f"[Registry Override] {meta}")
        destination = await self._llm_decide_destination()

        # 2) LLMìœ¼ë¡œ ìµœì¢… íŒŒë¼ë¯¸í„° ì‘ì„± (ë„êµ¬ ìŠ¤í‚¤ë§ˆ ì „ë‹¬, ê²½ë¡œëª… í•˜ë“œì½”ë”©/ê²€ì¶œ ì—†ìŒ)
        tool_spec = self.tool_manager.tool_catalog.get(action, {})
        authored = await self._llm_author_parameters(
            tool_name=action,
            tool_spec=tool_spec,
            destination=destination,
            proposed_params=action_input or {}
        )

        # 3) authored íŒŒë¼ë¯¸í„°ë¡œ ì¹˜í™˜ (ì´ì œë¶€í„°ëŠ” ì´ ê°’ë§Œ ì‚¬ìš©)
        action_input = authored or {}
        # 5) ë„êµ¬ ì‹¤í–‰
        if action in self.tool_manager.action_handlers:
            handler = self.tool_manager.action_handlers[action]
            # backup_file : ê°™ì€ íŒŒì¼ì€ ì„¸ì…˜ ë‚´ ì¤‘ë³µ ë°±ì—… ê¸ˆì§€
            try:
                if action == "backup_file":
                    action, action_input = await self._pivot_duplicate_backup(action, action_input)

                # ì‹¤í–‰
                observation_result = await self._execute_tool(action, action_input or {})

                # íš¨ê³¼ ì •ê·œí™” ë° ìƒíƒœ ê°±ì‹ 
                eff = self._normalize_effect(action, observation_result)
                if eff.kind == "file_backup":
                    self._backed_up_paths.add(eff.primary_path)
                elif eff.kind == "file_mutation":
                    self._written_paths.add(eff.primary_path)
                    await self._post_write_boost_effect(eff)  # â˜… ì—¬ê¸°ì„œë§Œ 1íšŒ í˜¸ì¶œ
            except Exception as _e:
                print(f"[Backup Guard Warning] {str(_e)}")
                # backup_file ì„±ê³µ ì‹œ ì„¸ì…˜ ê°€ë“œì— ì¶”ê°€
                try:
                    if action == "backup_file" and isinstance(observation_result, dict) and observation_result.get("status") == "ok":
                        from pathlib import Path as _P
                        _src = observation_result.get("src") or (action_input or {}).get("file_path") or (action_input or {}).get("src")
                        if _src:
                            self._backed_up_paths.add(str(_P(_src).resolve()))
                except Exception as _e:
                    print(f"[Backup Guard Update Warning] {str(_e)}")
            # ë ˆì§€ìŠ¤íŠ¸ë¦¬ í•¸ë“¤ ì¤€ë¹„(ì§€ì—°/ì„±ê³µ ë³´ê³ ìš©)
            try:
                from tool_registry import ToolRegistry
                if not hasattr(self, "_tool_registry") or self._tool_registry is None:
                    self._tool_registry = ToolRegistry()
                else:
                    self._tool_registry.refresh()
            except Exception:
                pass

            # 5-a) (ì„ íƒ) ìŠ¤í‚¤ë§ˆ ê²€ì¦
            do_schema_check = hasattr(self, "_validate_action_input") and hasattr(self, "tool_definitions")
            if do_schema_check:
                try:
                    ok, schema_err = self._validate_action_input(action, action_input or {})
                except Exception as e:
                    ok, schema_err = True, {}
                    print(f"[Schema Validate Warning] {e}")
                if not ok:
                    observation_result = schema_err

            if observation_result is None:
                # kwargsë¡œ í˜¸ì¶œ + ì§€ì—° ì¸¡ì •
                import time as _time, inspect as _inspect
                start_ts = _time.perf_counter()
                try:
                    sig = _inspect.signature(handler)
                    observation_result = await self._execute_tool(action, action_input or {})
                except TypeError as e:
                    try:
                        sig_str = str(_inspect.signature(handler))
                    except Exception:
                        sig_str = "(signature unavailable)"
                    observation_result = {
                        "status": "error",
                        "reason": f"í•¸ë“¤ëŸ¬ ì¸ì ë¶ˆì¼ì¹˜ë¡œ ì‹¤í–‰ ì‹¤íŒ¨: {e}",
                        "hint": "Use exactly the parameter names from generated_definitions.json.",
                        "handler_signature": sig_str,
                        "given_params": action_input or {}
                    }
                except Exception as e:
                    observation_result = {"status": "error", "reason": f"í•¸ë“¤ëŸ¬ ì‹¤í–‰ ì¤‘ ì˜ˆì™¸: {e}"}

        else:
            observation_result = {"status": "error", "reason": f"ì•Œ ìˆ˜ ì—†ëŠ” Agent ì•¡ì…˜: '{action}'."}

        print(f"[âœ… Observation] {json.dumps(observation_result, ensure_ascii=False, indent=2)}")
        await self._on_action_observed(action, action_input or {}, observation_result or {})
        # ë„êµ¬ ì‹¤í–‰ â†’ observation_result ì¶œë ¥ ì§í›„ì— ì•„ë˜ë¥¼ ì¶”ê°€
        try:
            if self._is_longform_goal() and self._extract_written_file_path(action, action_input or {}, observation_result or {}):
                await self._post_write_boost(action, action_input or {}, observation_result or {})
        except Exception as e:
            print(f"[PostWrite] continuation error: {e}")
        effect = self._normalize_effect(observation_result or {})
        self._effects.append(effect)

        try:
            if effect.kind == "file_mutation" and effect.ok:
                await self._post_write_boost_effect(effect)

                # ê¸¸ì´ ì¶©ì¡± ì‹œ ì¡°ê¸° ì¢…ë£Œ
                try:
                    from metatools.tool_selector import estimate_length_hint
                    min_chars, _ = estimate_length_hint(self.user_goal or "", "longform_text")
                    cur = _P(effect.primary_path).read_text(encoding="utf-8", errors="ignore")
                    if len(cur) >= min_chars:
                        ans = f"ì €ì¥ ì™„ë£Œ: {effect.primary_path}"
                        return True, thought, action, {"answer": ans}, {"answer": ans}
                except Exception:
                    pass
        except Exception as _e:
            print(f"[PostWrite Hook] {str(_e)}")
        # -- Post-write continuation hook: writeê°€ ë„ˆë¬´ ì§§ìœ¼ë©´ ìë™ ë³´ê°•(append) --
        try:
            written_path = self._extract_written_file_path(action, action_input or {}, observation_result or {})
            if written_path:
                from pathlib import Path as _P
                self._written_paths.add(str(_P(written_path).resolve()))
                self.last_written_file = written_path or self.last_written_file
        except Exception:
            pass
        try:
            if action == "backup_file" and isinstance(observation_result, dict) \
            and observation_result.get("status") == "ok":
                from pathlib import Path as _P
                _src = observation_result.get("src") or (action_input or {}).get("file_path") \
                    or (action_input or {}).get("src")
                if _src:
                    self._backed_up_paths.add(str(_P(_src).resolve()))
        except Exception as _e:
            print(f"[Backup Guard Update Warning] {str(_e)}")
        # -------------------------------------------------------------------------
        # 6) ë¶€ê°€ ìƒíƒœ ê°±ì‹ 
        try:
            if action == "write_file":
                # ìš°ì„  ê´€ì°° ê²°ê³¼(detail.path ë“±)ì—ì„œ ê²½ë¡œë¥¼ ì½ê³ , ì—†ìœ¼ë©´ ì…ë ¥ ì¸ì ì¤‘ 'path' í¬í•¨ í‚¤ë¥¼ íƒìƒ‰
                obs = observation_result if isinstance(observation_result, dict) else {}
                det = (obs.get("detail") or {}) if isinstance(obs.get("detail"), dict) else {}
                candidate = det.get("path") or obs.get("path")
                if not candidate:
                    for k, v in (action_input or {}).items():
                        if isinstance(k, str) and "path" in k.lower():
                            candidate = v
                            break
                self.last_written_file = candidate or ""
        except Exception:
            pass

        # 7) í”„ë¡¬í”„íŠ¸ì— í•¨ìˆ˜ì½œ/ì‘ë‹µ ê¸°ë¡
        try:
            if candidate:
                self.prompt_content.append(self._as_content(candidate))
            else:
                self.prompt_content.append(
                    types.Content(
                        role="model",
                        parts=[types.Part(function_call=types.FunctionCall(name=action, args=action_input or {}))]
                    )
                )
            self.prompt_content.append(
                types.Content(
                    role="user",
                    parts=[types.Part(function_response=types.FunctionResponse(name=action, response={"result": observation_result}))]
                )
            )
        except NameError:
            pass

        # 8) ë°±ê·¸ë¼ìš´ë“œ ìµœì í™” íŠ¸ë¦¬ê±°
        try:
            import asyncio
            if not hasattr(self, "optim_manager") or self.optim_manager is None:
                from optimization_manager import OptimizationManager
                self.optim_manager = OptimizationManager()
            generate_tool_definition_and_code = globals().get("generate_tool_definition_and_code")
            register_newly_generated_tool = globals().get("register_newly_generated_tool")
            handle_reload_skills = globals().get("handle_reload_skills")
            if generate_tool_definition_and_code and register_newly_generated_tool and handle_reload_skills:
                workflow_str_list = getattr(self, "session_workflow", [])
                user_goal_for_optimization = getattr(self, "user_goal", None)
                asyncio.create_task(
                    self.optim_manager.record_and_maybe_materialize(
                        workflow_str_list,
                        user_goal_for_optimization,
                        generate_tool_definition_and_code=generate_tool_definition_and_code,
                        register_newly_generated_tool=register_newly_generated_tool,
                        handle_reload_skills=handle_reload_skills
                    )
                )
        except Exception as e:
            print(f"[Optimization Hook Error] {e}")

        # 9) ë‹¤ìŒ ì‚¬ì´í´ë¡œ ì§„í–‰
        return False, thought, action, action_input, observation_result
    def _save_tool_call_log(self, iteration: int, tool_name: str, parameters: dict):
        """DBì— íˆ´ í˜¸ì¶œ ê¸°ë¡ì„ ë™ê¸°ì ìœ¼ë¡œ ì €ì¥í•©ë‹ˆë‹¤."""
        try:
            new_log = ToolCallLog(
                session_id=self.session_id, request_id=self.request_id, iteration=iteration,
                tool_name=tool_name, parameters=parameters, timestamp=datetime.datetime.now()
            )
            self.db.add(new_log); self.db.commit()
            print(f"[ğŸ“Š Tool Log Saved] Session: {self.session_id}, Request: {self.request_id}, Tool: {tool_name}")
        except Exception as e:
            self.db.rollback()
            print(f"[âŒ Tool Log Error] íˆ´ ë¡œê·¸ DB ì €ì¥ ì‹¤íŒ¨: {e}")
    async def stream_run(self):
        """
        ì—ì´ì „íŠ¸ì˜ ì „ì²´ ì‹¤í–‰ì„ ìŠ¤íŠ¸ë¦¬ë°í•˜ëŠ” ë©”ì¸ ì œë„ˆë ˆì´í„°.
        """
        try:
            yield f"data: {json.dumps({'type': 'session_info', 'session_id': self.session_id})}\n\n"
            await self._initialize()

            # 1. ì‹¤í–‰ ì „, ìµœì ì˜ 'ì²« ë‹¨ê³„'ë¥¼ ì œì•ˆë°›ìŠµë‹ˆë‹¤.
            first_step_proposal = await self._propose_first_step()

            # 2. ë©”ì¸ ë£¨í”„ë¥¼ ì‹¤í–‰í•©ë‹ˆë‹¤.
            for i in range(MAX_AGENT_ITERATIONS):
                # ì²« ë²ˆì§¸ ì´í„°ë ˆì´ì…˜ì—ë§Œ ì œì•ˆì„ ì „ë‹¬í•©ë‹ˆë‹¤.
                proposal_for_this_iteration = first_step_proposal if i == 0 else (None, None)
                
                is_final, thought, action, action_input, observation_result = await self._run_iteration(
                    i, first_step_proposal=proposal_for_this_iteration
                )
                
                if thought:
                    async for chunk in stream_text_by_char('thought_stream', thought): yield chunk
                    yield f"data: {json.dumps({'type': 'thought_stream_end'})}\n\n"
                
                if is_final:
                    obs = observation_result or {}
                    if not isinstance(obs, dict):
                        obs = {"status": "error", "reason": str(obs)}
                    final_answer = (action_input or {}).get("answer") or obs.get("answer") or "ì‘ì—…ì´ ì™„ë£Œë˜ì—ˆìŠµë‹ˆë‹¤."
                    if action == "error":
                        async for chunk in stream_text_by_char('error_stream', final_answer): yield chunk
                    else:
                        if len(self.session_workflow) >= 2:
                            print(f"âœ¨ ìµœì í™” ê°€ëŠ¥ì„± ë°œê²¬! ì›Œí¬í”Œë¡œìš° ê¸¸ì´: {len(self.session_workflow)}. ìµœì í™” í”„ë¡œì„¸ìŠ¤ë¥¼ ì‹œì‘í•©ë‹ˆë‹¤.")
                            self.background_tasks.add_task(run_optimization_workflow, {"completed_workflow": self.session_workflow, "user_goal": self.user_goal})
                        
                        await asyncio.to_thread(save_message, self.session_id, self.user_goal, self.db, self.current_user_id)
                        await asyncio.to_thread(save_message, self.session_id, final_answer, self.db, None)
                        try:
                            if getattr(self, "_materialize_queue", None):
                                for spec in self._materialize_queue:
                                    # ë„¤ ìª½ ìµœì í™” ë§¤ë‹ˆì € APIì— ë§ê²Œ ë°”ê¿”ì„œ í˜¸ì¶œ
                                    self.optim_manager.record_materialize_proposal(spec)
                        except Exception as _e:
                            print(f"[materialize] ì œì•ˆ ì „ë‹¬ ì‹¤íŒ¨: {_e}")
                        
                        async for chunk in stream_text_by_char('final_answer_stream', final_answer): yield chunk
                    
                    break

        except Exception as e:
            error_msg = f"Agent ì‹¤í–‰ ì¤‘ ì‹¬ê°í•œ ì˜¤ë¥˜ ë°œìƒ: {e}"
            print(f"âŒ {error_msg}")
            import traceback; traceback.print_exc()
            async for chunk in stream_text_by_char('error_stream', error_msg): yield chunk
        finally:
            self.db.close()
            print(f"[DB Session Closed] Session for {self.session_id} has been closed.")
        
        yield f"data: {json.dumps({'type': 'final_stream_end', 'lastWrittenFile': self.last_written_file})}\n\n"
        yield "data: [DONE]\n\n"
    def _load_tool_definitions(self) -> dict:
        """
        generated_definitions.jsonì„ ì½ì–´ ê° toolì˜ í—ˆìš©/í•„ìˆ˜ íŒŒë¼ë¯¸í„°ë¥¼ ìºì‹œ.
        """
        defs_path = BACKEND_DIR / "tools" / "generated_definitions.json"
        if not defs_path.exists():
            return {}
        try:
            data = json.loads(defs_path.read_text(encoding="utf-8"))
        except Exception:
            return {}
        tools = {}
        for t in data.get("tools", []):
            name = t.get("name")
            params = t.get("parameters", {})
            props = params.get("properties", {}) if isinstance(params, dict) else {}
            required = set(params.get("required", [])) if isinstance(params, dict) else set()
            if name:
                tools[name] = {"allowed": set(props.keys()), "required": required}
        return tools
    def _looks_like_path_or_url(self, s: str) -> bool:
        if not isinstance(s, str): return False
        s = s.strip()
        if s.startswith("http://") or s.startswith("https://"):
            return True
        # íŒŒì¼ ê²½ë¡œ/í™•ì¥ì í”ì 
        if re.search(r"[\\/]", s) and re.search(r"\.[A-Za-z0-9]{1,5}($|\?)", s):
            return True
        return False
    def _build_user_message(self, text: str):
        """
        ë¬¸ìì—´ promptë¥¼ Gemini APIê°€ ë°›ëŠ” messages í¬ë§·ìœ¼ë¡œ ë³€í™˜.
        google.genai.types ê°€ ìˆìœ¼ë©´ ê·¸ê²ƒì„, ì—†ìœ¼ë©´ dict í¬ë§·ì„ ì‚¬ìš©.
        """
        try:
            from google.genai import types
            return [types.Content(role="user", parts=[types.Part(text=text)])]
        except Exception:
            return [{"role": "user", "parts": [{"text": text}]}]
    async def _select_payload_key_via_llm(self, tool_spec: dict, action_input: dict) -> str | None:
        """
        LLMì—ê²Œ 'ë©”ì¸ í…ìŠ¤íŠ¸ í˜ì´ë¡œë“œ'ë¡œ ì í•©í•œ ë¬¸ìì—´ í•„ë“œ 1ê°œë¥¼ ì„ íƒì‹œí‚¨ë‹¤.
        - ìŠ¤í‚¤ë§ˆ(properties/description/type)ì™€ í˜„ì¬ ê°’ ìƒ˜í”Œì„ ë³´ì—¬ì£¼ê³ 
        - íŒŒì¼ ê²½ë¡œ/URL/íƒ€ì´í‹€/í† í”½ ë“±ì€ ì œì™¸í•˜ë„ë¡ ì§€ì‹œ
        - JSON {"key":"..."}ë§Œ ë°˜í™˜
        """
        params = (tool_spec or {}).get("parameters", {}) or {}
        props  = (params.get("properties") or {}) if isinstance(params.get("properties"), dict) else {}
        # ë¬¸ìì—´ í•„ë“œë§Œ ì¶”ì¶œ
        cand = {k:v for k,v in props.items() if isinstance(v, dict) and v.get("type") == "string"}
        if not cand:
            return None

        # ìƒ˜í”Œ ê°’ê³¼ ì„¤ëª… ì œê³µ
        sample = {k: (action_input.get(k) if isinstance(action_input.get(k), str) else "") for k in cand.keys()}
        schema_brief = {k: {"description": cand[k].get("description",""), "type": "string"} for k in cand.keys()}

        instruction = (
            "Pick the single property key that holds the MAIN TEXT PAYLOAD to be written.\n"
            "- Exclude paths, URLs, titles, topics, or identifiers.\n"
            "- Prefer the field that contains the document/article/html/code body.\n"
            "Return JSON only: {\"key\":\"<property_name>\"}"
        )
        prompt = f"{instruction}\n\nSCHEMA:\n{json.dumps(schema_brief, ensure_ascii=False, indent=2)}\n\nSAMPLES:\n{json.dumps(sample, ensure_ascii=False, indent=2)}"

        resp, _ = await self._call_llm_safe(self._build_user_message(prompt), use_tools=False)
        text = getattr(resp, "text", None) if not isinstance(resp, str) else resp
        try:
            obj = json.loads((text or "").strip())
            key = obj.get("key")
            return key if key in cand else None
        except Exception:
            return None

    def _fallback_payload_key(self, tool_spec: dict, action_input: dict) -> str | None:
        """
        LLM ì„ íƒ ì‹¤íŒ¨ì‹œ: ë¬¸ìì—´ í•„ë“œ ì¤‘ ê²½ë¡œ/URLë¡œ ë³´ì´ëŠ” ê°’ì€ ì œì™¸í•˜ê³ ,
        í˜„ì¬ ê°’ì´ ê°€ì¥ ê¸´ í‚¤ë¥¼ ì„ íƒ. ëª¨ë‘ ë¹„ë©´ ì„ì˜ì˜ ì²« í‚¤.
        """
        params = (tool_spec or {}).get("parameters", {}) or {}
        props  = (params.get("properties") or {}) if isinstance(params.get("properties"), dict) else {}
        cands  = [k for k,v in props.items() if isinstance(v, dict) and v.get("type") == "string"]
        if not cands: return None

        filtered = []
        for k in cands:
            val = action_input.get(k, "")
            if isinstance(val, str) and self._looks_like_path_or_url(val):
                continue
            filtered.append(k)
        if not filtered:
            filtered = cands

        best_k, best_len = None, -1
        for k in filtered:
            v = action_input.get(k, "")
            L = len(v) if isinstance(v, str) else -1
            if L > best_len:
                best_k, best_len = k, L
        return best_k if best_k is not None else filtered[0]

    async def _enrich_preserving_format(self, current_text: str, user_goal: str, min_chars: int) -> str:
        """
        í˜•ì‹(HTML/Markdown/ì½”ë“œ/í‰ë¬¸)ì„ ìœ ì§€í•˜ë©° 'current_text'ë¥¼ ì¶©ë¶„íˆ í™•ì¥.
        ë©”íƒ€ë©˜íŠ¸ ì—†ì´ ê²°ê³¼ë¬¼ë§Œ.
        """
        instruction = (
            "Expand the CURRENT_TEXT while strictly preserving its format/type.\n"
            "- If HTML: output valid HTML (doctype optional if already present).\n"
            "- If code: output only the code in the same language.\n"
            "- If Markdown: keep Markdown.\n"
            "- If plain text: produce a longer high-quality article in the same language.\n"
            "- DO NOT add any explanations or meta text; output content only.\n"
            f"- Target length â‰¥ {min_chars} characters.\n"
        )
        prompt = f"{instruction}\n\n[USER_GOAL]\n{user_goal}\n\n[CURRENT_TEXT]\n{current_text or ''}\n"
        resp, _ = await self._call_llm_safe(self._build_user_message(prompt), use_tools=False)
        text = getattr(resp, "text", None) if not isinstance(resp, str) else resp
        return (text or "").strip()

    async def _maybe_enrich_text_payload_strong(self, *, action: str, action_input: dict, tool_spec: dict, min_threshold: int, min_chars: int) -> dict:
        key = self._pick_payload_field_fast(tool_spec, action_input or {})
        if not key: return action_input
        cur = action_input.get(key, "")
        if not isinstance(cur, str) or len(cur.strip()) >= min_threshold:
            return action_input

        print(f"[ENRICH] start action={action}, key=content, len={len((action_input or {}).get('content',''))} â†’ targetâ‰¥{min_chars}")
        enriched = await self._compose_or_enrich(self.user_goal or "", cur, min_chars=min_chars)
        print(f"[ENRICH] done len={len((action_input or {}).get('content',''))}")
        if enriched and len(enriched) > len(cur):
            print(f"[ENRICH] done key={key}, len {len(cur)} â†’ {len(enriched)}")
            new_args = dict(action_input or {})
            new_args[key] = enriched
            return new_args
        
        print(f"[ENRICH] fail/no-change key={key}, kept len={len(cur)}")
        return action_input
    def _validate_action_input(self, action: str, action_input: dict) -> tuple[bool, dict]:
        """
        í˜¸ì¶œ ì§ì „ ìŠ¤í‚¤ë§ˆ ê²€ì¦.
        - unknown_keys / missing_keys ìˆìœ¼ë©´ False ë°˜í™˜ + íŒíŠ¸ í¬í•¨
        """
        if not hasattr(self, "tool_definitions") or not self.tool_definitions:
            try:
                self.tool_definitions = self._load_tool_definitions()
            except Exception:
                self.tool_definitions = {}

        schema = self.tool_definitions.get(action)
        if not schema:
            # ì •ì˜ ì—†ëŠ” ë„êµ¬(ë ˆê±°ì‹œ)ë©´ í†µê³¼
            return True, {}

        allowed = schema.get("allowed", set())
        required = schema.get("required", set())

        keys = set((action_input or {}).keys())
        unknown = keys - allowed
        missing = required - keys

        if unknown or missing:
            return False, {
                "status": "schema_error",
                "unknown_keys": sorted(list(unknown)),
                "missing_keys": sorted(list(missing)),
                "hint": f"Use only keys {sorted(list(allowed))} for tool '{action}'."
            }
        return True, {}
    def _current_plan_tools(self) -> list[str]:
        tools = []
        for s in getattr(self, "session_workflow", []):
            try:
                obj = json5.loads(s)
                name = obj.get("tool_name")
                if isinstance(name, str) and name:
                    tools.append(name)
            except: 
                continue
        return tools
    def _maybe_override_with_registry_tool(self, action: str, action_input: dict):
        try:
            plan_steps = self._get_planned_steps() or [action]
            catalog = self.tool_manager.tool_catalog

            def _ensure_args(name: str, params: dict):
                try:
                    ok, norm, err = self.tool_manager.ensure_args(name, params or {})
                    return ok, (norm or {}), (err or None)
                except Exception as e:
                    return False, {}, str(e)

            pick = select_best_covering_tool(plan_steps, catalog, _ensure_args, min_cover_len=1)
            if not pick:
                return action, action_input, {"overridden": False}

            picked_tool, base_args = pick

            # content/ì¶œë ¥ ê²½ë¡œëŠ” ìŠ¹ê³„.
            merged = dict(base_args or {})
            if "content" in (action_input or {}):
                merged["content"] = action_input["content"]

            # âœ… file_pathëŠ” "ì‚¬ìš©ìê°€ ëª…ì‹œí•œ ê²½ìš°"ì—ë§Œ ìŠ¹ê³„. (LLMì´ ì„ì˜ ìƒì„±í•œ ê²½ë¡œëŠ” ë²„ë¦¼)
            if "file_path" in (action_input or {}):
                if self._user_explicitly_requested_path(action_input["file_path"]):
                    merged["file_path"] = action_input["file_path"]

            # âœ… output_path: ì´ê²ƒë„ ì‚¬ìš©ì ëª…ì‹œì¼ ë•Œë§Œ ìŠ¹ê³„ (LLMì´ ì„ì˜ ìƒì„±í•œ ê²½ë¡œëŠ” ë¬´ì‹œ)
            if "output_path" in (action_input or {}):
                if self._user_explicitly_requested_path(action_input["output_path"]):
                    merged["output_path"] = action_input["output_path"]

            ok, normed, err = _ensure_args(picked_tool, merged)
            if not ok:
                return action, action_input, {"overridden": False, "reason": f"ensure_args failed: {err}"}

            return picked_tool, normed, {"overridden": True, "by": "coverage"}
        except Exception as e:
            return action, action_input, {"overridden": False, "error": str(e)}
    def _coerce_json_from_text(self, raw_text: str) -> dict | None:
        """
        1) prompt_builder.extract_json_from_text() ì‹œë„
        2) ì‹¤íŒ¨ ì‹œ ë°”ê¹¥ { ... } ê°•íƒˆí•´ì„œ json5 íŒŒì‹±
        """
        try:
            from prompt_builder import extract_json_from_text
        except Exception:
            extract_json_from_text = None

        text = (raw_text or "").strip()
        if not text:
            return None

        # 1) í”„ë¡œì íŠ¸ ì œê³µ íŒŒì„œ
        if extract_json_from_text:
            try:
                cand = extract_json_from_text(text)
                if cand:
                    obj = json5.loads(cand)
                    if isinstance(obj, dict):
                        return obj
            except Exception:
                pass

        # 2) ë°”ê¹¥ ì¤‘ê´„í˜¸ ê°•íƒˆ
        try:
            l = text.find("{")
            r = text.rfind("}")
            if l != -1 and r != -1 and r > l:
                cand2 = text[l:r+1]
                obj2 = json5.loads(cand2)
                if isinstance(obj2, dict):
                    return obj2
        except Exception:
            pass

        return None
    async def _repair_json_via_llm(self, raw_text: str, max_attempts: int = 3) -> dict | None:
        """
        ëª¨ë¸ì´ ìœ íš¨ JSONì„ ë‚´ë†“ì§€ ëª»í–ˆì„ ë•Œ, ë³´ì • ì „ìš© í”„ë¡¬í”„íŠ¸ë¡œ ìµœëŒ€ NíšŒ ë³µêµ¬.
        - ì¶œë ¥ì€ ë‹¨ì¼ JSON ê°ì²´ë§Œ í—ˆìš©.
        - êµ¬ì¡° ê°•ì œ: {"Thought": "...", "Action": {"tool_name": "...", "parameters": {...}}}
        """
        if not raw_text:
            return None

        REPAIR_INSTRUCTION = (
            "ì•„ë˜ì˜ ì›ë³¸ ì‘ë‹µì„ ê·œì¹™ì— ë§ëŠ” ë‹¨ì¼ JSON ê°ì²´ë¡œ ë³´ì •í•˜ì—¬ ë°˜í™˜í•˜ì„¸ìš”.\n"
            "- ì¶œë ¥ì€ ì˜¤ì§ JSON ê°ì²´ 1ê°œë§Œ(ì½”ë“œë¸”ë¡/ì£¼ì„/ì„¤ëª… ê¸ˆì§€)\n"
            '- êµ¬ì¡°: {"Thought":"...","Action":{"tool_name":"...","parameters":{}}}\n'
            "- ë¬¸ìì—´ì˜ ë”°ì˜´í‘œ/ì¤„ë°”ê¿ˆ/ë°±í‹±ì€ JSON ë¬¸ë²•ì— ë§ê²Œ ì´ìŠ¤ì¼€ì´í”„\n"
            "- ëˆ„ë½ëœ ë‹«ëŠ” ê´„í˜¸/ë”°ì˜´í‘œ/ì‰¼í‘œ ë³´ì •\n"
            "- parameters ì•ˆì˜ thought/debug/notes ë“± ë©”íƒ€í‚¤ ì œê±°\n"
            "- ì´ë¯¸ ìœ íš¨í•œ JSONì´ë©´ ê·¸ëŒ€ë¡œ ì¬ì¶œë ¥\n"
        )

        for _ in range(max_attempts):
            prompt = (
                REPAIR_INSTRUCTION
                + "\n\n[ì›ë³¸ ì‘ë‹µ]\n<<<\n" + raw_text + "\n>>>\n"
            )
            try:
                response, _ = await self._call_llm_safe(
                    [types.Content(role="user", parts=[types.Part(text=prompt)])],
                    available_models=getattr(self, "available_models", None),
                    use_tools=False
                )
            except Exception as e:
                print(f"[JSON Repair Warning] LLM call failed: {e}")
                await asyncio.sleep(0.8)
                continue

            text = getattr(response, "text", None)
            if not text:
                await asyncio.sleep(0.4)
                continue

            obj = self._coerce_json_from_text(text)
            if isinstance(obj, dict):
                act = obj.get("Action")
                if isinstance(act, dict) and isinstance(act.get("tool_name"), str) and "parameters" in act:
                    return obj

            await asyncio.sleep(0.4)

        return None
